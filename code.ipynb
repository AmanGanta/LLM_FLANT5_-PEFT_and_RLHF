{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8760516,"sourceType":"datasetVersion","datasetId":5263436}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:04.352992Z","iopub.execute_input":"2024-06-29T17:52:04.353319Z","iopub.status.idle":"2024-06-29T17:52:08.521950Z","shell.execute_reply.started":"2024-06-29T17:52:04.353289Z","shell.execute_reply":"2024-06-29T17:52:08.521026Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### INSTALATIONS","metadata":{}},{"cell_type":"code","source":"%pip install -U datasets==2.17.0 torch==1.13.1 torchdata==0.5.1 transformers==4.27.2 evaluate==0.4.0 rouge_score==0.1.2 loralib==0.1.1 peft==0.3.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-29T17:50:00.902769Z","iopub.execute_input":"2024-06-29T17:50:00.903420Z","iopub.status.idle":"2024-06-29T17:52:04.350755Z","shell.execute_reply.started":"2024-06-29T17:50:00.903387Z","shell.execute_reply":"2024-06-29T17:52:04.349580Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting datasets==2.17.0\n  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\nCollecting torch==1.13.1\n  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\nCollecting torchdata==0.5.1\n  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting transformers==4.27.2\n  Downloading transformers-4.27.2-py3-none-any.whl.metadata (106 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting evaluate==0.4.0\n  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\nCollecting rouge_score==0.1.2\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting loralib==0.1.1\n  Downloading loralib-0.1.1-py3-none-any.whl.metadata (14 kB)\nCollecting peft==0.3.0\n  Downloading peft-0.3.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.16)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.0) (6.0.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.13.1) (4.9.0)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.5.1) (1.26.18)\nCollecting portalocker>=2.0.0 (from torchdata==0.5.1)\n  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.2) (2023.12.25)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.2)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting responses<0.19 (from evaluate==0.4.0)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score==0.1.2) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.16.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.3.0) (5.9.3)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.3.0) (0.30.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (69.0.3)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.42.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.17.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2024.2.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate->peft==0.3.0) (0.4.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.4)\nDownloading datasets-2.17.0-py3-none-any.whl (536 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m766.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\nDownloading peft-0.3.0-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m416.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.0-py3-none-any.whl (18 kB)\nDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=933ab688684576a4a6403d67f476d858271485012edeb852bb70020cdd9b7390\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: tokenizers, portalocker, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, loralib, fsspec, rouge_score, responses, nvidia-cudnn-cu11, transformers, torch, torchdata, datasets, peft, evaluate\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.3.1\n    Uninstalling fsspec-2024.3.1:\n      Successfully uninstalled fsspec-2024.3.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n  Attempting uninstall: torchdata\n    Found existing installation: torchdata 0.7.1\n    Uninstalling torchdata-0.7.1:\n      Successfully uninstalled torchdata-0.7.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.19.2\n    Uninstalling datasets-2.19.2:\n      Successfully uninstalled datasets-2.19.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngcsfs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\nkaggle-environments 1.14.11 requires transformers>=4.33.1, but you have transformers 4.27.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.17.0 evaluate-0.4.0 fsspec-2023.10.0 loralib-0.1.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 peft-0.3.0 portalocker-2.10.0 responses-0.18.0 rouge_score-0.1.2 tokenizers-0.13.3 torch-1.13.1 torchdata-0.5.1 transformers-4.27.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndata = \"knkarthick/dialogsum\"\ndt = load_dataset(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:19.912016Z","iopub.execute_input":"2024-06-29T17:52:19.912931Z","iopub.status.idle":"2024-06-29T17:52:23.208449Z","shell.execute_reply.started":"2024-06-29T17:52:19.912895Z","shell.execute_reply":"2024-06-29T17:52:23.206563Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"463fe30939ef4e70934f4f1856e5ed2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d98f92f92d5436492a0abc9016143f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"825c69faa35c4784bae404fd147e2081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"700d3e4ac6a0402289c338e98095aa22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74e023c289c34b24a7cfd024df63f6f0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"837e253efc3b46c99084d2759486e968"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd100f2a62a49d0b7699a5c850c0ae1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset size and columns available ","metadata":{"execution":{"iopub.status.busy":"2024-06-12T01:40:21.951011Z","iopub.execute_input":"2024-06-12T01:40:21.951422Z","iopub.status.idle":"2024-06-12T01:40:21.955735Z","shell.execute_reply.started":"2024-06-12T01:40:21.951387Z","shell.execute_reply":"2024-06-12T01:40:21.954883Z"}}},{"cell_type":"code","source":"dt","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:28.635222Z","iopub.execute_input":"2024-06-29T17:52:28.635912Z","iopub.status.idle":"2024-06-29T17:52:28.642988Z","shell.execute_reply.started":"2024-06-29T17:52:28.635878Z","shell.execute_reply":"2024-06-29T17:52:28.642059Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"ds = [12460, 500, 1500]\nlb = ['Training Set', 'Validation Set', 'Test Set']\ndef cal(p, a):\n    ab = int(p/100.*sum(a))\n    return \"{:.1f}%\\n({:d})\".format(p, ab)\nplt.figure(figsize=(8, 8))\nplt.pie(ds, labels=lb, autopct=lambda pct: cal(pct, ds))\nplt.title('Data Splits Size')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:30.762364Z","iopub.execute_input":"2024-06-29T17:52:30.763197Z","iopub.status.idle":"2024-06-29T17:52:31.023020Z","shell.execute_reply.started":"2024-06-29T17:52:30.763161Z","shell.execute_reply":"2024-06-29T17:52:31.021683Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"train_df = pd.DataFrame(dt['train'])\nvalidation_df = pd.DataFrame(dt['validation'])\ntest_df = pd.DataFrame(dt['test'])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:33.410431Z","iopub.execute_input":"2024-06-29T17:52:33.410788Z","iopub.status.idle":"2024-06-29T17:52:34.325569Z","shell.execute_reply.started":"2024-06-29T17:52:33.410763Z","shell.execute_reply":"2024-06-29T17:52:34.324561Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Looking at Data","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:36.495651Z","iopub.execute_input":"2024-06-29T17:52:36.496393Z","iopub.status.idle":"2024-06-29T17:52:36.511466Z","shell.execute_reply.started":"2024-06-29T17:52:36.496361Z","shell.execute_reply":"2024-06-29T17:52:36.510398Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        id                                           dialogue  \\\n0  train_0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n1  train_1  #Person1#: Hello Mrs. Parker, how have you bee...   \n2  train_2  #Person1#: Excuse me, did you see a set of key...   \n3  train_3  #Person1#: Why didn't you tell me you had a gi...   \n4  train_4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n\n                                             summary              topic  \n0  Mr. Smith's getting a check-up, and Doctor Haw...     get a check-up  \n1  Mrs Parker takes Ricky for his vaccines. Dr. P...           vaccines  \n2  #Person1#'s looking for a set of keys and asks...          find keys  \n3  #Person1#'s angry because #Person2# didn't tel...  have a girlfriend  \n4  Malik invites Nikki to dance. Nikki agrees if ...              dance  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>dialogue</th>\n      <th>summary</th>\n      <th>topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...</td>\n      <td>Mr. Smith's getting a check-up, and Doctor Haw...</td>\n      <td>get a check-up</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>#Person1#: Hello Mrs. Parker, how have you bee...</td>\n      <td>Mrs Parker takes Ricky for his vaccines. Dr. P...</td>\n      <td>vaccines</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>#Person1#: Excuse me, did you see a set of key...</td>\n      <td>#Person1#'s looking for a set of keys and asks...</td>\n      <td>find keys</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>#Person1#: Why didn't you tell me you had a gi...</td>\n      <td>#Person1#'s angry because #Person2# didn't tel...</td>\n      <td>have a girlfriend</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>#Person1#: Watsup, ladies! Y'll looking'fine t...</td>\n      <td>Malik invites Nikki to dance. Nikki agrees if ...</td>\n      <td>dance</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(train_df['dialogue'][0])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:38.300128Z","iopub.execute_input":"2024-06-29T17:52:38.300477Z","iopub.status.idle":"2024-06-29T17:52:38.305713Z","shell.execute_reply.started":"2024-06-29T17:52:38.300451Z","shell.execute_reply":"2024-06-29T17:52:38.304672Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n#Person2#: I found it would be a good idea to get a check-up.\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n#Person2#: Ok.\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n#Person2#: Yes.\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n#Person2#: Ok, thanks doctor.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_df['summary'][0])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:38.665252Z","iopub.execute_input":"2024-06-29T17:52:38.666099Z","iopub.status.idle":"2024-06-29T17:52:38.671145Z","shell.execute_reply.started":"2024-06-29T17:52:38.666064Z","shell.execute_reply":"2024-06-29T17:52:38.669999Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dropping unnecessary column for our training ","metadata":{}},{"cell_type":"code","source":"train_df.drop('topic',axis=1,inplace=True)\nvalidation_df.drop('topic',axis=1,inplace=True)\ntest_df.drop('topic',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:48.609305Z","iopub.execute_input":"2024-06-29T17:52:48.609719Z","iopub.status.idle":"2024-06-29T17:52:48.622325Z","shell.execute_reply.started":"2024-06-29T17:52:48.609687Z","shell.execute_reply":"2024-06-29T17:52:48.621450Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:52:49.384779Z","iopub.execute_input":"2024-06-29T17:52:49.385159Z","iopub.status.idle":"2024-06-29T17:52:49.396774Z","shell.execute_reply.started":"2024-06-29T17:52:49.385128Z","shell.execute_reply":"2024-06-29T17:52:49.395674Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"        id                                           dialogue  \\\n0  train_0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n1  train_1  #Person1#: Hello Mrs. Parker, how have you bee...   \n2  train_2  #Person1#: Excuse me, did you see a set of key...   \n3  train_3  #Person1#: Why didn't you tell me you had a gi...   \n4  train_4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n\n                                             summary  \n0  Mr. Smith's getting a check-up, and Doctor Haw...  \n1  Mrs Parker takes Ricky for his vaccines. Dr. P...  \n2  #Person1#'s looking for a set of keys and asks...  \n3  #Person1#'s angry because #Person2# didn't tel...  \n4  Malik invites Nikki to dance. Nikki agrees if ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>dialogue</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...</td>\n      <td>Mr. Smith's getting a check-up, and Doctor Haw...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>#Person1#: Hello Mrs. Parker, how have you bee...</td>\n      <td>Mrs Parker takes Ricky for his vaccines. Dr. P...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>#Person1#: Excuse me, did you see a set of key...</td>\n      <td>#Person1#'s looking for a set of keys and asks...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>#Person1#: Why didn't you tell me you had a gi...</td>\n      <td>#Person1#'s angry because #Person2# didn't tel...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>#Person1#: Watsup, ladies! Y'll looking'fine t...</td>\n      <td>Malik invites Nikki to dance. Nikki agrees if ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"og = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\ntk = AutoTokenizer.from_pretrained('google/flan-t5-base')","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:07:35.728040Z","iopub.execute_input":"2024-06-29T18:07:35.728518Z","iopub.status.idle":"2024-06-29T18:07:46.576992Z","shell.execute_reply.started":"2024-06-29T18:07:35.728470Z","shell.execute_reply":"2024-06-29T18:07:46.575720Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e9191207aa47d584f894a4ab3c46f4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8fc11ffca4a4e9e96a795117dd2f2b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee664f26b6374f3f9eee7f728bb3a4d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"975ccbe279894c26a31c66dae9df0593"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bbc1256fa1d4194a2a76aff83f80950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c0f49cb94a4b01b5c3d4f570a4cdfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf936f453f044897abe4ebac5254a301"}},"metadata":{}}]},{"cell_type":"code","source":"a = \"My name is aman\"\nen = tk(a)\ns = tk.decode(\n        en[\"input_ids\"][0])\nprint(s)\nprint(en)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:40:26.107728Z","iopub.execute_input":"2024-06-29T15:40:26.108013Z","iopub.status.idle":"2024-06-29T15:40:36.005699Z","shell.execute_reply.started":"2024-06-29T15:40:26.107988Z","shell.execute_reply":"2024-06-29T15:40:36.004665Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2024-06-29 15:40:27.771583: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-29 15:40:27.771683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-29 15:40:27.897274: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"My\n{'input_ids': [499, 564, 19, 3, 9, 348, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Lets test some random prompts","metadata":{}},{"cell_type":"code","source":"#https://huggingface.co/docs/transformers/en/model_doc/flan-t5","metadata":{"execution":{"iopub.status.busy":"2024-06-22T16:30:57.179204Z","iopub.execute_input":"2024-06-22T16:30:57.180181Z","iopub.status.idle":"2024-06-22T16:30:57.185242Z","shell.execute_reply.started":"2024-06-22T16:30:57.180141Z","shell.execute_reply":"2024-06-22T16:30:57.184031Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"p=\"Write an essay about India\"\nip = tk(p, return_tensors=\"pt\")\not = og.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\nre = tk.decode(ot[0], skip_special_tokens=True)\nprint(f\"Prompt: {p}\")\nprint(f\"Response: {re}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T16:30:57.566700Z","iopub.execute_input":"2024-06-22T16:30:57.567050Z","iopub.status.idle":"2024-06-22T16:31:09.205728Z","shell.execute_reply.started":"2024-06-22T16:30:57.567023Z","shell.execute_reply":"2024-06-22T16:31:09.204711Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Prompt: Write an essay about India\nResponse: India is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country\n\n","output_type":"stream"}]},{"cell_type":"code","source":"p=\"Summarize this in 1 lines: India is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country\"\nip = tk(p, return_tensors=\"pt\")\not = og.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\nre = tk.decode(ot[0], skip_special_tokens=True)\nprint(f\"Prompt: {p}\\n\")\nprint(f\"Response: {re}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T16:31:10.333165Z","iopub.execute_input":"2024-06-22T16:31:10.334138Z","iopub.status.idle":"2024-06-22T16:31:12.212237Z","shell.execute_reply.started":"2024-06-22T16:31:10.334101Z","shell.execute_reply":"2024-06-22T16:31:12.211291Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Prompt: Summarize this in 1 lines: India is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country with a lot of history and culture. It is a country\n\nResponse: India is a country with a lot of history and culture.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Trying different prompt styles for the model","metadata":{}},{"cell_type":"code","source":"p=\"summarize  \\n\" + train_df['dialogue'][30]\nip = tk(p, return_tensors=\"pt\")\not = og.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\nre = tk.decode(ot[0], skip_special_tokens=True)\nprint(f\"Prompt: {p}\")\nprint(f\"Response: {re}\\n\")\nprint(\"original summary \"+train_df['summary'][30])","metadata":{"execution":{"iopub.status.busy":"2024-06-22T16:31:13.589190Z","iopub.execute_input":"2024-06-22T16:31:13.589530Z","iopub.status.idle":"2024-06-22T16:31:14.947888Z","shell.execute_reply.started":"2024-06-22T16:31:13.589504Z","shell.execute_reply":"2024-06-22T16:31:14.946981Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Prompt: summarize  \n#Person1#: May I take your order?\n#Person2#: We haven't decided yet. Could you give us a little longer?\n#Person1#: Yes, take your time, please.\n#Person2#: Can we get something to drink? We want two bottles of beer.\n#Person1#: Fine.\n#Person2#: Could you tell us your specials today?\n#Person1#: The special today is steak.\n#Person2#: We'll take this steak dinner.\n#Person1#: What would you like to go with your steak?\n#Person2#: Peas and carrots.\n#Person1#: I see. What would you like for dessert?\n#Person2#: Icecream, please.\nResponse: #Person1#: We're going to take the steak dinner.\n\noriginal summary #Person1# serves #Person2# to order two bottles of beer, a steak dinner, and ice cream.\n","output_type":"stream"}]},{"cell_type":"code","source":"\np=\"Explain whats happening  \\n\" + train_df['dialogue'][30]\nip = tk(p, return_tensors=\"pt\")\not = og.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\nre = tk.decode(ot[0], skip_special_tokens=True)\nprint(f\"Prompt: {p}\")\nprint(f\"Response: {re}\\n\")\nprint(\"original summary \"+train_df['summary'][30])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:50:16.529445Z","iopub.execute_input":"2024-06-21T13:50:16.530668Z","iopub.status.idle":"2024-06-21T13:50:17.536842Z","shell.execute_reply.started":"2024-06-21T13:50:16.530625Z","shell.execute_reply":"2024-06-21T13:50:17.535850Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Prompt: Explain whats happening  \n#Person1#: May I take your order?\n#Person2#: We haven't decided yet. Could you give us a little longer?\n#Person1#: Yes, take your time, please.\n#Person2#: Can we get something to drink? We want two bottles of beer.\n#Person1#: Fine.\n#Person2#: Could you tell us your specials today?\n#Person1#: The special today is steak.\n#Person2#: We'll take this steak dinner.\n#Person1#: What would you like to go with your steak?\n#Person2#: Peas and carrots.\n#Person1#: I see. What would you like for dessert?\n#Person2#: Icecream, please.\nResponse: The restaurant is full.\n\noriginal summary #Person1# serves #Person2# to order two bottles of beer, a steak dinner, and ice cream.\n","output_type":"stream"}]},{"cell_type":"code","source":"p=\"Summarize the conversation\\n\" + train_df['dialogue'][30]\nip = tk(p, return_tensors=\"pt\")\not = og.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\nre = tk.decode(ot[0], skip_special_tokens=True)\nprint(f\"Prompt: {p}\\n\")\nprint(f\"Response: {re}\\n\")\nprint(\"original summary \"+train_df['summary'][30])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:50:17.538319Z","iopub.execute_input":"2024-06-21T13:50:17.538807Z","iopub.status.idle":"2024-06-21T13:50:20.906784Z","shell.execute_reply.started":"2024-06-21T13:50:17.538779Z","shell.execute_reply":"2024-06-21T13:50:20.905864Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Prompt: Summarize the conversation\n#Person1#: May I take your order?\n#Person2#: We haven't decided yet. Could you give us a little longer?\n#Person1#: Yes, take your time, please.\n#Person2#: Can we get something to drink? We want two bottles of beer.\n#Person1#: Fine.\n#Person2#: Could you tell us your specials today?\n#Person1#: The special today is steak.\n#Person2#: We'll take this steak dinner.\n#Person1#: What would you like to go with your steak?\n#Person2#: Peas and carrots.\n#Person1#: I see. What would you like for dessert?\n#Person2#: Icecream, please.\n\nResponse: The steak dinner will be served at 7:00 pm.\n\noriginal summary #Person1# serves #Person2# to order two bottles of beer, a steak dinner, and ice cream.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Lets try one shot inference method ","metadata":{}},{"cell_type":"code","source":"p=\"Summarize the conversation\\n\"+train_df['dialogue'][200] + \"\\nSummary:\" + train_df['summary'][200] +\"\\n\\nSummarize the conversation\\n\"+ train_df['dialogue'][30] \nip = tk(p, return_tensors=\"pt\")\not = og.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\nre = tk.decode(ot[0], skip_special_tokens=True)\nprint(f\"Prompt: {p}\\n\")\nprint(f\"Response: {re}\\n\")\nprint(\"original summary \"+train_df['summary'][30])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:50:20.908297Z","iopub.execute_input":"2024-06-21T13:50:20.908671Z","iopub.status.idle":"2024-06-21T13:50:23.624120Z","shell.execute_reply.started":"2024-06-21T13:50:20.908638Z","shell.execute_reply":"2024-06-21T13:50:23.623173Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Prompt: Summarize the conversation\n#Person1#: What do you want to know about me?\n#Person2#: How about your academic records at college?\n#Person1#: The average grade of all my courses is above 85.\n#Person2#: In which subject did you get the highest marks?\n#Person1#: In mathematics I got a 98.\n#Person2#: Have you received any scholarships?\n#Person1#: Yes, I have, and three times in total.\n#Person2#: Have you been a class leader?\n#Person1#: I have been a class commissary in charge of studies for two years.\n#Person2#: Did you join in any club activities?\n#Person1#: I was an aerobics team member in college.\n#Person2#: What sport are you good at?\n#Person1#: I am good at sprint and table tennis.\n#Person2#: You are excellent.\nSummary:#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n\nSummarize the conversation\n#Person1#: May I take your order?\n#Person2#: We haven't decided yet. Could you give us a little longer?\n#Person1#: Yes, take your time, please.\n#Person2#: Can we get something to drink? We want two bottles of beer.\n#Person1#: Fine.\n#Person2#: Could you tell us your specials today?\n#Person1#: The special today is steak.\n#Person2#: We'll take this steak dinner.\n#Person1#: What would you like to go with your steak?\n#Person2#: Peas and carrots.\n#Person1#: I see. What would you like for dessert?\n#Person2#: Icecream, please.\n\nResponse: Summary: #Person1#: We're going to take the steak dinner order.\n\noriginal summary #Person1# serves #Person2# to order two bottles of beer, a steak dinner, and ice cream.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Try fewshot prompting ","metadata":{}},{"cell_type":"code","source":"a=\"\"\nfor i in ([2,44,66,111]):\n    a=a+\"Summarize the conversation\\n\"+train_df['dialogue'][i] + \"\\nSummary:\" + train_df['summary'][i]+\"\\n\\n\"","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:50:23.625314Z","iopub.execute_input":"2024-06-21T13:50:23.625636Z","iopub.status.idle":"2024-06-21T13:50:23.630340Z","shell.execute_reply.started":"2024-06-21T13:50:23.625611Z","shell.execute_reply":"2024-06-21T13:50:23.629440Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"p=a+\"\\n\\nSummarize the conversation\\n\"+ train_df['dialogue'][30] \nip = tk(p, return_tensors=\"pt\")\not = og.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\nre = tk.decode(ot[0], skip_special_tokens=True)\nprint(f\"Prompt: {p}\\n\")\nprint(f\"Response: {re}\\n\")\nprint(\"original summary \"+train_df['summary'][30])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:50:23.631458Z","iopub.execute_input":"2024-06-21T13:50:23.631727Z","iopub.status.idle":"2024-06-21T13:50:31.342982Z","shell.execute_reply.started":"2024-06-21T13:50:23.631705Z","shell.execute_reply":"2024-06-21T13:50:31.342012Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Summarize the conversation\n#Person1#: Excuse me, did you see a set of keys?\n#Person2#: What kind of keys?\n#Person1#: Five keys and a small foot ornament.\n#Person2#: What a shame! I didn't see them.\n#Person1#: Well, can you help me look for it? That's my first time here.\n#Person2#: Sure. It's my pleasure. I'd like to help you look for the missing keys.\n#Person1#: It's very kind of you.\n#Person2#: It's not a big deal.Hey, I found them.\n#Person1#: Oh, thank God! I don't know how to thank you, guys.\n#Person2#: You're welcome.\nSummary:#Person1#'s looking for a set of keys and asks for #Person2#'s help to find them.\n\nSummarize the conversation\n#Person1#: Next week is your birthday party. How exciting! What do you want for your birthday?\n#Person2#: Well, a car like yours would be fantastic.\n#Person1#: Hmm...Maybe not a car just yet. You're only turning 9, you know? How about a doll?\n#Person2#: Mom, I'm a boy. Boys don't play with stupid dolls, but a machine gun would be OK, too.\n#Person1#: Well, not this time. Maybe we could get you a nice dress and a little purse, son. Maybe...\n#Person2#: Maybe I should change my name to Mary since you seem to want a daughter so badly?\nSummary:#Person2# wants a cool birthday present, but his mom wants to give him a sissy present.\n\nSummarize the conversation\n#Person1#: Good morning, Maintenance Department.\n#Person2#: Hello. I'm having a problem with my air conditioner.\n#Person1#: Which air conditioner?\n#Person2#: The one in the bedroom.\n#Person1#: What seems to be the problem?\n#Person2#: There's no cold air coming out.\n#Person1#: May I have your room number, please?\n#Person2#: 512.\n#Person1#: OK, we'll send someone up to check it.\n#Person2#: I'm going out right now. But that's all right, you can come when I've gone.\n#Person1#: Fine. The housekeeper will open the door and stay in the room with the repairmen.\nSummary:#Person2# phones Maintenance Department because the air conditioner went wrong. #Person1# answers the phone and will send the repairmen.\n\nSummarize the conversation\n#Person1#: I really want to take a nap. I feel very sleepy today.\n#Person2#: What's the matter? Didn't you get enough sleep last night?\n#Person1#: I fell asleep very late. It was almost two o'clock in the morning when I finally fell asleep.\n#Person2#: Are you worried about something? Why couldn't you sleep?\n#Person1#: You know how it is when you're in a strange country. Everything is new, and you get tired and nervous sometimes. Then you worry about your family, about conditions back home, about your courses, about your money, about everything. I tried to fall asleep but I just had too much on my mind.\n#Person2#: Well, take it easy. Things will look better tomorrow. Maybe you should try exercising or a hot bath to help you relax.\n#Person1#: Anything is worth a try. But right now I really just want to find a quiet place to take a nap.\nSummary:#Person1# fell asleep very late because #Person1# had too much on the mind. #Person2# gives #Person1# some suggestions but #Person1# only wants to take a nap right now.\n\n\n\nSummarize the conversation\n#Person1#: May I take your order?\n#Person2#: We haven't decided yet. Could you give us a little longer?\n#Person1#: Yes, take your time, please.\n#Person2#: Can we get something to drink? We want two bottles of beer.\n#Person1#: Fine.\n#Person2#: Could you tell us your specials today?\n#Person1#: The special today is steak.\n#Person2#: We'll take this steak dinner.\n#Person1#: What would you like to go with your steak?\n#Person2#: Peas and carrots.\n#Person1#: I see. What would you like for dessert?\n#Person2#: Icecream, please.\n\nResponse: Summary: #Person1#: We're going to have a steak dinner.\n\noriginal summary #Person1# serves #Person2# to order two bottles of beer, a steak dinner, and ice cream.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### So now lets try to fine tune the model to give better results by PEFT","metadata":{}},{"cell_type":"markdown","source":"#### before going forward lets test base models score","metadata":{}},{"cell_type":"code","source":"#rd_f = test_df.sample(n=10, random_state=1)\nds = test_df['dialogue']\nhm = test_df['summary']","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:40:36.007118Z","iopub.execute_input":"2024-06-29T15:40:36.008202Z","iopub.status.idle":"2024-06-29T15:40:36.013140Z","shell.execute_reply.started":"2024-06-29T15:40:36.008164Z","shell.execute_reply":"2024-06-29T15:40:36.012099Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nog.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:40:36.014246Z","iopub.execute_input":"2024-06-29T15:40:36.014609Z","iopub.status.idle":"2024-06-29T15:40:37.259946Z","shell.execute_reply.started":"2024-06-29T15:40:36.014579Z","shell.execute_reply":"2024-06-29T15:40:37.259002Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (8): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (9): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (10): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (11): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (8): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (9): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (10): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (11): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"os = []\nfor i, j in enumerate(ds):\n    p = \"Summarize the conversation\\n\" + j\n    ip = tk(p, return_tensors=\"pt\").to(device)\n\n    ogo = og.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\n    re = tk.decode(ogo[0], skip_special_tokens=True)\n\n    os.append(re)\ndf = pd.DataFrame({\n    'human summaries': hm,\n    'model summary': os\n})\n\ndf","metadata":{"execution":{"iopub.status.busy":"2024-06-22T16:35:20.109261Z","iopub.execute_input":"2024-06-22T16:35:20.109645Z","iopub.status.idle":"2024-06-22T16:47:09.731429Z","shell.execute_reply.started":"2024-06-22T16:35:20.109617Z","shell.execute_reply":"2024-06-22T16:47:09.730537Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1023 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                                        human summaries  \\\n0     Ms. Dawson helps #Person1# to write a memo to ...   \n1     In order to prevent employees from wasting tim...   \n2     Ms. Dawson takes a dictation for #Person1# abo...   \n3     #Person2# arrives late because of traffic jam....   \n4     #Person2# decides to follow #Person1#'s sugges...   \n...                                                 ...   \n1495  Matthew and Steve meet after a long time. Stev...   \n1496  Steve has been looking for a place to live. Ma...   \n1497  Frank invites Besty to the party to celebrate ...   \n1498  Frank invites Betsy to the big promotion party...   \n1499  Frank invites Betsy to his party for his promo...   \n\n                                          model summary  \n0     The following memo is to be distributed to all...  \n1     The following memo is to be distributed to all...  \n2     The following memo is to be distributed to all...  \n3     The traffic jam at the Carrefour intersection ...  \n4     The traffic jam at the Carrefour intersection ...  \n...                                                 ...  \n1495                              #Person1#: Hi, Steve.  \n1496                              #Person1#: Hi, Steve.  \n1497  Person1 is going to throw a party for all of h...  \n1498  Person1 is going to throw a party for all of h...  \n1499  Person1 is going to throw a party for all of h...  \n\n[1500 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human summaries</th>\n      <th>model summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>The following memo is to be distributed to all...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>The following memo is to be distributed to all...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>The following memo is to be distributed to all...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>Matthew and Steve meet after a long time. Stev...</td>\n      <td>#Person1#: Hi, Steve.</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>Steve has been looking for a place to live. Ma...</td>\n      <td>#Person1#: Hi, Steve.</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>Frank invites Besty to the party to celebrate ...</td>\n      <td>Person1 is going to throw a party for all of h...</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>Frank invites Betsy to the big promotion party...</td>\n      <td>Person1 is going to throw a party for all of h...</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>Frank invites Betsy to his party for his promo...</td>\n      <td>Person1 is going to throw a party for all of h...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import evaluate\nr = evaluate.load('rouge')\nore = r.compute(\n    predictions=df['model summary'].tolist(),\n    references=df['human summaries'].tolist(),\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint(ore)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T16:47:39.496279Z","iopub.execute_input":"2024-06-22T16:47:39.497108Z","iopub.status.idle":"2024-06-22T16:47:45.858346Z","shell.execute_reply.started":"2024-06-22T16:47:39.497076Z","shell.execute_reply":"2024-06-22T16:47:45.857265Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eedfda94ea34ee1a5cd6a90ceda3e57"}},"metadata":{}},{"name":"stdout","text":"{'rouge1': 0.2248748468148314, 'rouge2': 0.06455773069170524, 'rougeL': 0.19279672363512235, 'rougeLsum': 0.192602292931872}\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nmetrics = list(ore.keys())\nscores = list(ore.values())\nplt.figure(figsize=(10, 6))\nplt.bar(metrics, scores, color='skyblue')\nplt.xlabel('ROUGE Metrics')\nplt.ylabel('Scores')\nplt.title('ROUGE Scores for Original Model Summaries')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T16:47:50.653843Z","iopub.execute_input":"2024-06-22T16:47:50.654571Z","iopub.status.idle":"2024-06-22T16:47:50.904367Z","shell.execute_reply.started":"2024-06-22T16:47:50.654527Z","shell.execute_reply":"2024-06-22T16:47:50.903283Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"### Beginning the training process","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"LORA\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:40:37.261134Z","iopub.execute_input":"2024-06-29T15:40:37.261428Z","iopub.status.idle":"2024-06-29T15:52:46.704120Z","shell.execute_reply.started":"2024-06-29T15:40:37.261404Z","shell.execute_reply":"2024-06-29T15:52:46.702916Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240629_155228-9jh9aqvh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lora-project/LORA/runs/9jh9aqvh' target=\"_blank\">floral-tree-3</a></strong> to <a href='https://wandb.ai/lora-project/LORA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lora-project/LORA' target=\"_blank\">https://wandb.ai/lora-project/LORA</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lora-project/LORA/runs/9jh9aqvh' target=\"_blank\">https://wandb.ai/lora-project/LORA/runs/9jh9aqvh</a>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lora-project/LORA/runs/9jh9aqvh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7d891b537040>"},"metadata":{}}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\ncongfi = LoraConfig(\n    r=16, \n    lora_alpha=16,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:51:38.867280Z","iopub.execute_input":"2024-06-21T13:51:38.867602Z","iopub.status.idle":"2024-06-21T13:51:38.890697Z","shell.execute_reply.started":"2024-06-21T13:51:38.867576Z","shell.execute_reply":"2024-06-21T13:51:38.889632Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"pm = get_peft_model(og, congfi)\ntp = sum(p.numel() for p in pm.parameters() if p.requires_grad)\nttp = sum(p.numel() for p in pm.parameters())\nprint(\"For PEFT MODEL usign LORA\",tp)\nprint(\"Percentage of Trainable Parameters:\", 100 * tp / ttp)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:51:38.892117Z","iopub.execute_input":"2024-06-21T13:51:38.892523Z","iopub.status.idle":"2024-06-21T13:51:39.595686Z","shell.execute_reply.started":"2024-06-21T13:51:38.892491Z","shell.execute_reply":"2024-06-21T13:51:39.594653Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"For PEFT MODEL usign LORA 1769472\nPercentage of Trainable Parameters: 0.7096414524241463\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\ntrg = TrainingArguments(\n    output_dir='./op',  \n    auto_find_batch_size=True,\n    learning_rate=1e-3,  \n    num_train_epochs=10,\n    logging_steps=500,\n    report_to=\"wandb\"\n     \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:51:39.597000Z","iopub.execute_input":"2024-06-21T13:51:39.598091Z","iopub.status.idle":"2024-06-21T13:51:39.665937Z","shell.execute_reply.started":"2024-06-21T13:51:39.598048Z","shell.execute_reply":"2024-06-21T13:51:39.664864Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ndef fun(dtt):\n    ip = [\"Summarize the conversation\\n\" + dialogue for dialogue in dtt['dialogue']]\n    mp = tk(ip, truncation=True, padding=\"max_length\")\n    with tk.as_target_tokenizer():\n        labels = tk(dtt['summary'], truncation=True, padding=\"max_length\")\n    mp[\"labels\"] = labels[\"input_ids\"]\n    return mp\n\nDat = Dataset.from_pandas(train_df)\ndt_tk = Dat.map(fun, batched=True)\ndt_tk_t = dt_tk","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:51:39.667048Z","iopub.execute_input":"2024-06-21T13:51:39.667329Z","iopub.status.idle":"2024-06-21T13:51:47.746018Z","shell.execute_reply.started":"2024-06-21T13:51:39.667304Z","shell.execute_reply":"2024-06-21T13:51:47.745034Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24757e06ff394202adab8f93ca37d09c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3586: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"d = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npm.to(d)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:51:47.747190Z","iopub.execute_input":"2024-06-21T13:51:47.747504Z","iopub.status.idle":"2024-06-21T13:51:48.778666Z","shell.execute_reply.started":"2024-06-21T13:51:47.747466Z","shell.execute_reply":"2024-06-21T13:51:48.777454Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): T5ForConditionalGeneration(\n      (shared): Embedding(32128, 768)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"#### TRAINING","metadata":{"execution":{"iopub.status.busy":"2024-07-06T18:14:58.919386Z","iopub.execute_input":"2024-07-06T18:14:58.920129Z","iopub.status.idle":"2024-07-06T18:14:58.924955Z","shell.execute_reply.started":"2024-07-06T18:14:58.920096Z","shell.execute_reply":"2024-07-06T18:14:58.923863Z"}}},{"cell_type":"code","source":"tr = Trainer(\n    model=pm,\n    args=trg,\n    train_dataset=dt_tk,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:51:48.780234Z","iopub.execute_input":"2024-06-21T13:51:48.780674Z","iopub.status.idle":"2024-06-21T13:51:48.797472Z","shell.execute_reply.started":"2024-06-21T13:51:48.780633Z","shell.execute_reply":"2024-06-21T13:51:48.796441Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module='transformers.optimization')\ntr.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T13:51:48.798696Z","iopub.execute_input":"2024-06-21T13:51:48.799179Z","iopub.status.idle":"2024-06-21T19:12:58.404876Z","shell.execute_reply.started":"2024-06-21T13:51:48.799153Z","shell.execute_reply":"2024-06-21T19:12:58.403984Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15580' max='15580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15580/15580 5:21:07, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.834500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.126700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.118000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.112000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.111000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.106900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.103400</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.104200</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.102500</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.099700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.098000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.099000</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.098000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.094400</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.096700</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.095700</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.094100</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.092400</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.094000</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.090300</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.092700</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.091900</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.090800</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.089400</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.089600</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.087200</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.089600</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.089400</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.086700</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.088100</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.087400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15580, training_loss=0.15302278374217748, metrics={'train_runtime': 19269.2209, 'train_samples_per_second': 6.466, 'train_steps_per_second': 0.809, 'total_flos': 8.59980690358272e+16, 'train_loss': 0.15302278374217748, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"### SAVING AND EVALUATING MODEL","metadata":{}},{"cell_type":"code","source":"tr.model.save_pretrained(\"./peft-dialogue\")\ntk.save_pretrained(\"./peft-dialogue\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T19:32:38.505651Z","iopub.execute_input":"2024-06-21T19:32:38.506267Z","iopub.status.idle":"2024-06-21T19:32:38.593598Z","shell.execute_reply.started":"2024-06-21T19:32:38.506234Z","shell.execute_reply":"2024-06-21T19:32:38.591440Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"('./peft-dialogue/tokenizer_config.json',\n './peft-dialogue/special_tokens_map.json',\n './peft-dialogue/spiece.model',\n './peft-dialogue/added_tokens.json',\n './peft-dialogue/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"peft-dialogue\", 'zip', \"./peft-dialogue\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T19:34:03.350223Z","iopub.execute_input":"2024-06-21T19:34:03.350632Z","iopub.status.idle":"2024-06-21T19:34:03.911393Z","shell.execute_reply.started":"2024-06-21T19:34:03.350604Z","shell.execute_reply":"2024-06-21T19:34:03.910432Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/peft-dialogue.zip'"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'peft-dialogue.zip')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T19:34:17.743650Z","iopub.execute_input":"2024-06-21T19:34:17.744003Z","iopub.status.idle":"2024-06-21T19:34:17.751853Z","shell.execute_reply.started":"2024-06-21T19:34:17.743975Z","shell.execute_reply":"2024-06-21T19:34:17.750732Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/peft-dialogue.zip","text/html":"<a href='peft-dialogue.zip' target='_blank'>peft-dialogue.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel\ntk = AutoTokenizer.from_pretrained(\"/kaggle/input/peft-dialogue\")\npm = PeftModel.from_pretrained(og, \"/kaggle/input/peft-dialogue\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npm.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T15:54:39.879314Z","iopub.execute_input":"2024-06-29T15:54:39.879976Z","iopub.status.idle":"2024-06-29T15:54:40.919307Z","shell.execute_reply.started":"2024-06-29T15:54:39.879948Z","shell.execute_reply":"2024-06-29T15:54:40.918248Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): T5ForConditionalGeneration(\n      (shared): Embedding(32128, 768)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### ROUGH FOR PEFT MODEL ON TEST DATA","metadata":{}},{"cell_type":"code","source":"ds = test_df['dialogue']\nhm = test_df['summary']\n\nos = []\nfor i, j in enumerate(ds):\n    p = \"Summarize the conversation\\n\" + j\n    ip = tk(p, return_tensors=\"pt\").to(device)\n    ogo = pm.model.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\n    re = tk.decode(ogo[0], skip_special_tokens=True)\n\n    os.append(re)\ndf = pd.DataFrame({\n    'human summaries': hm,\n    'model summary': os\n})\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T20:01:09.433858Z","iopub.execute_input":"2024-06-21T20:01:09.434581Z","iopub.status.idle":"2024-06-21T20:26:51.045156Z","shell.execute_reply.started":"2024-06-21T20:01:09.434550Z","shell.execute_reply":"2024-06-21T20:26:51.044082Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1023 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"\nrouge = evaluate.load('rouge')\nore = rouge.compute(\n    predictions=df['model summary'].tolist(),\n    references=df['human summaries'].tolist(),\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint(ore)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T20:28:48.504284Z","iopub.execute_input":"2024-06-21T20:28:48.504952Z","iopub.status.idle":"2024-06-21T20:28:53.612255Z","shell.execute_reply.started":"2024-06-21T20:28:48.504923Z","shell.execute_reply":"2024-06-21T20:28:53.611040Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"{'rouge1': 0.4534539494105222, 'rouge2': 0.19174333083749437, 'rougeL': 0.36820616832548786, 'rougeLsum': 0.36823502596311425}\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nmetrics = list(ore.keys())\nscores = list(ore.values())\nplt.figure(figsize=(10, 6))\nplt.bar(metrics, scores, color='skyblue')\nplt.xlabel('ROUGE Metrics')\nplt.ylabel('Scores')\nplt.title('ROUGE Scores for Original Model Summaries')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T20:30:08.283578Z","iopub.execute_input":"2024-06-21T20:30:08.284267Z","iopub.status.idle":"2024-06-21T20:30:08.521934Z","shell.execute_reply.started":"2024-06-21T20:30:08.284237Z","shell.execute_reply":"2024-06-21T20:30:08.520916Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nsteps = [\n    500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000,\n    5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000,\n    10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000,\n    15500\n]\n\ntraining_loss = [\n    1.834500, 0.126700, 0.118000, 0.112000, 0.111000, 0.106900, 0.103400, 0.104200, 0.102500, 0.099700,\n    0.098000, 0.099000, 0.098000, 0.094400, 0.096700, 0.095700, 0.094100, 0.092400, 0.094000, 0.090300,\n    0.092700, 0.091900, 0.090800, 0.089400, 0.089600, 0.087200, 0.089600, 0.089400, 0.086700, 0.088100,\n    0.087400\n]\n\nplt.figure(figsize=(10, 6))\nplt.plot(steps, training_loss, marker='o', linestyle='-', color='b')\nplt.title('Training Loss Over Steps')\nplt.xlabel('Steps')\nplt.ylabel('Training Loss')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T16:50:59.639380Z","iopub.execute_input":"2024-06-22T16:50:59.639785Z","iopub.status.idle":"2024-06-22T16:50:59.893577Z","shell.execute_reply.started":"2024-06-22T16:50:59.639754Z","shell.execute_reply":"2024-06-22T16:50:59.892599Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"### RESULTS COMPARISION ","metadata":{"execution":{"iopub.status.busy":"2024-07-06T18:15:54.952127Z","iopub.execute_input":"2024-07-06T18:15:54.952736Z","iopub.status.idle":"2024-07-06T18:15:54.956609Z","shell.execute_reply.started":"2024-07-06T18:15:54.952705Z","shell.execute_reply":"2024-07-06T18:15:54.955634Z"}}},{"cell_type":"code","source":"import numpy as np\n\nl_scores = {\n    'rouge1': 0.453,\n    'rouge2': 0.192,\n    'rougeL': 0.368,\n    'rougeLsum': 0.368\n}\n\no_scores = {\n    'rouge1': 0.225,\n    'rouge2': 0.065,\n    'rougeL': 0.193,\n    'rougeLsum': 0.193\n}\n\nlabels = list(l_scores.keys())\nl_vals = list(l_scores.values())\no_vals = list(o_scores.values())\n\nx = np.arange(len(labels))\nw = 0.35\n\nfig, ax = plt.subplots(figsize=(10, 6))\nr1 = ax.bar(x - w/2, l_vals, w, label='LORA Trained Model', color='b')\nr2 = ax.bar(x + w/2, o_vals, w, label='Original Model', color='g')\n\nax.set_xlabel('ROUGE Scores')\nax.set_ylabel('Values')\nax.set_title('Comparison of ROUGE Scores between LORA Trained Model and Original Model')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\ndef annotate_bars(rects):\n    for rect in rects:\n        h = rect.get_height()\n        ax.annotate(f'{h:.3f}', xy=(rect.get_x() + rect.get_width() / 2, h),\n                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n\nannotate_bars(r1)\nannotate_bars(r2)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T16:58:54.752189Z","iopub.execute_input":"2024-06-22T16:58:54.752937Z","iopub.status.idle":"2024-06-22T16:58:55.061496Z","shell.execute_reply.started":"2024-06-22T16:58:54.752901Z","shell.execute_reply":"2024-06-22T16:58:55.060591Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel\ntk = AutoTokenizer.from_pretrained(\"/kaggle/input/peft-dialogue\")\npm = PeftModel.from_pretrained(og, \"/kaggle/input/peft-dialogue\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npm.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:48:43.737636Z","iopub.execute_input":"2024-06-22T18:48:43.738451Z","iopub.status.idle":"2024-06-22T18:48:46.020009Z","shell.execute_reply.started":"2024-06-22T18:48:43.738420Z","shell.execute_reply":"2024-06-22T18:48:46.019031Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): T5ForConditionalGeneration(\n      (shared): Embedding(32128, 768)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"p=\"summarize  \\n\" + test_df['dialogue'][30]\nip = tk(p, return_tensors=\"pt\").to(device)\not = pm.model.generate(ip[\"input_ids\"], max_length=300, num_return_sequences=1)\nre = tk.decode(ot[0], skip_special_tokens=True)\nprint(f\"Prompt: {p}\")\nprint(f\"Response: {re}\\n\")\nprint(\"original summary \"+test_df['summary'][30])","metadata":{"execution":{"iopub.status.busy":"2024-06-22T19:09:01.212536Z","iopub.execute_input":"2024-06-22T19:09:01.213164Z","iopub.status.idle":"2024-06-22T19:09:14.906873Z","shell.execute_reply.started":"2024-06-22T19:09:01.213122Z","shell.execute_reply":"2024-06-22T19:09:14.905826Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"2024-06-22 19:09:05.689364: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-22 19:09:05.689504: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-22 19:09:05.822822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Prompt: summarize  \n#Person1#: Where are you going for your trip?\n#Person2#: I think Hebei is a good place.\n#Person1#: But I heard the north of China are experiencing severe sandstorms!\n#Person2#: Really?\n#Person1#: Yes, it's said that Hebes was experiencing six degree strong winds.\n#Person2#: How do these storms affect the people who live in these areas?\n#Person1#: The report said the number of people with respiratory tract infections tended to rise after sandstorms. The sand gets into people's noses and throats and creates irritation.\n#Person2#: It sounds that sandstorms are trouble for everybody!\n#Person1#: You are quite right.\nResponse: #Person2#'s going to Hebei for a trip. #Person1# tells #Person2# the north of China is experiencing severe sandstorms. #Person2# thinks sandstorms are trouble for everybody.\n\noriginal summary #Person2# plans to have a trip in Hebei but #Person1# says there are sandstorms in there.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install git+https://github.com/lvwerra/trl.git@25fa1bd ","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:11:16.702233Z","iopub.execute_input":"2024-06-29T18:11:16.702644Z","iopub.status.idle":"2024-06-29T18:11:34.804545Z","shell.execute_reply.started":"2024-06-29T18:11:16.702611Z","shell.execute_reply":"2024-06-29T18:11:34.803073Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/lvwerra/trl.git@25fa1bd\n  Cloning https://github.com/lvwerra/trl.git (to revision 25fa1bd) to /tmp/pip-req-build-o58_955m\n  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-req-build-o58_955m\n\u001b[33m  WARNING: Did not find branch or tag '25fa1bd', assuming revision or ref.\u001b[0m\u001b[33m\n\u001b[0m  Running command git checkout -q 25fa1bd\n  Resolved https://github.com/lvwerra/trl.git to commit 25fa1bd\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (1.13.1)\nRequirement already satisfied: transformers>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (4.27.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (0.30.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.4.2.dev0) (2.17.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (4.9.0)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (69.0.3)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (0.42.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.23.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2.32.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.4.2.dev0) (5.9.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.4.2.dev0) (0.4.3)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (0.70.16)\nRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->trl==0.4.2.dev0) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.4.2.dev0) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.18.0->trl==0.4.2.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.4.2.dev0) (1.16.0)\nBuilding wheels for collected packages: trl\n  Building wheel for trl (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for trl: filename=trl-0.4.2.dev0-py3-none-any.whl size=67532 sha256=f84dec44e29c263f1c879d7fdfced442317030ce96cac86384107d0c04f20d52\n  Stored in directory: /tmp/pip-ephem-wheel-cache-17fvpfy6/wheels/24/b4/20/2fa3a1e47c0411c39e198029315e3af2a2c1d59132913f136f\nSuccessfully built trl\nInstalling collected packages: trl\nSuccessfully installed trl-0.4.2.dev0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### RLHF TRAINING PROCESS BEGINS ","metadata":{}},{"cell_type":"code","source":"from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\nfrom trl.core import LengthSampler","metadata":{"execution":{"iopub.status.busy":"2024-06-29T16:01:10.851653Z","iopub.execute_input":"2024-06-29T16:01:10.852614Z","iopub.status.idle":"2024-06-29T16:01:11.324757Z","shell.execute_reply.started":"2024-06-29T16:01:10.852576Z","shell.execute_reply":"2024-06-29T16:01:11.323720Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel\ntk = AutoTokenizer.from_pretrained(\"/kaggle/input/peft-dialogue\")\npm = PeftModel.from_pretrained(og, \"/kaggle/input/peft-dialogue\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npm.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\ntox_tk = AutoTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\ntox_mdl = AutoModelForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T16:21:42.834681Z","iopub.execute_input":"2024-06-29T16:21:42.835594Z","iopub.status.idle":"2024-06-29T16:21:48.377861Z","shell.execute_reply.started":"2024-06-29T16:21:42.835555Z","shell.execute_reply":"2024-06-29T16:21:48.376806Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df317afe7b3f428f8b828ac7fa9f4dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20188bb659be46738fe9909a9b412659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2196a1a5998c4d91bbc5eafeea66eff5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e68b0598cc4800b65002a2c1ac3186"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b62e48f55a4756a155d4396f772ec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e95168b5d464a48b6334c8b3062aedd"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\ndef eval_tox(md, tox_mdl, tox_tk, ds):\n    max_toks = 300\n    tox_scores = []\n    max_seq_len = tox_tk.model_max_length\n    for i, dlg in tqdm(enumerate(ds)):\n        ip = tk(f\"Summarize the conversation\\n{dlg}\", return_tensors=\"pt\").to(device)\n        gen_ids = md.model.generate(ip[\"input_ids\"], max_length=max_toks)\n        gen_txt = tk.decode(gen_ids[0], skip_special_tokens=True)\n\n        full_txt = dlg + \" \" + gen_txt\n        tox_ip_ids = tox_tk(full_txt, truncation=True,max_length=max_seq_len,return_tensors=\"pt\").input_ids.to(device)\n        logits = tox_mdl(tox_ip_ids).logits\n        probs = logits.softmax(dim=-1).tolist()[0]\n        tox_score = probs[1]  \n\n        tox_scores.append(tox_score)\n\n    mean_tox = np.mean(tox_scores)\n    std_tox = np.std(tox_scores)\n    \n    return mean_tox, std_tox\n\nds = test_df['dialogue']\nmean_tox, std_tox = eval_tox(pm, tox_mdl, tox_tk, ds)\nprint(f'Toxicity [mean, std]: [{mean_tox}, {std_tox}]')","metadata":{"execution":{"iopub.status.busy":"2024-06-29T16:32:35.901721Z","iopub.execute_input":"2024-06-29T16:32:35.902578Z","iopub.status.idle":"2024-06-29T16:59:11.969587Z","shell.execute_reply.started":"2024-06-29T16:32:35.902545Z","shell.execute_reply":"2024-06-29T16:59:11.968472Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"261it [04:28,  1.11s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1023 > 512). Running this sequence through the model will result in indexing errors\n1500it [26:36,  1.06s/it]","output_type":"stream"},{"name":"stdout","text":"Toxicity [mean, std]: [0.028158942559879507, 0.08178810660198951]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"##Training LLM on RLHF","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"og = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\ntk = AutoTokenizer.from_pretrained('google/flan-t5-base')","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:48:09.273812Z","iopub.execute_input":"2024-06-29T19:48:09.274621Z","iopub.status.idle":"2024-06-29T19:48:14.870571Z","shell.execute_reply.started":"2024-06-29T19:48:09.274583Z","shell.execute_reply":"2024-06-29T19:48:14.869645Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel\npeft_model_path = \"/kaggle/input/peft-dialogue\"\npm = PeftModel.from_pretrained(og, peft_model_path)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npm.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:48:14.872496Z","iopub.execute_input":"2024-06-29T19:48:14.872861Z","iopub.status.idle":"2024-06-29T19:48:15.880010Z","shell.execute_reply.started":"2024-06-29T19:48:14.872833Z","shell.execute_reply":"2024-06-29T19:48:15.878796Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): T5ForConditionalGeneration(\n      (shared): Embedding(32128, 768)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32128, 768)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                  (relative_attention_bias): Embedding(32, 12)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (2): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (3): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (4): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (5): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (6): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (7): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (8): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (9): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (10): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (11): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear(in_features=768, out_features=768, bias=False)\n                  (v): Linear(\n                    in_features=768, out_features=768, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear(in_features=768, out_features=768, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from trl import AutoModelForSeq2SeqLMWithValueHead\nppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(pm, torch_dtype=torch.bfloat16, is_trainable=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:48:15.881296Z","iopub.execute_input":"2024-06-29T19:48:15.881690Z","iopub.status.idle":"2024-06-29T19:48:15.913233Z","shell.execute_reply.started":"2024-06-29T19:48:15.881655Z","shell.execute_reply":"2024-06-29T19:48:15.911933Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"from trl import create_reference_model\nref_model = create_reference_model(ppo_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:48:42.719605Z","iopub.execute_input":"2024-06-29T19:48:42.719970Z","iopub.status.idle":"2024-06-29T19:48:43.316797Z","shell.execute_reply.started":"2024-06-29T19:48:42.719944Z","shell.execute_reply":"2024-06-29T19:48:43.315545Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"tp = sum(p.numel() for p in ppo_model.parameters() if p.requires_grad)\nttp = sum(p.numel() for p in ppo_model.parameters())\nprint(\"For PPO Model using PEFT\")\nprint(\"Trainable Parameters:\", tp)\nprint(\"Total Parameters:\", ttp)\nprint(\"Percentage of Trainable Parameters:\", 100 * tp / ttp)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:50:12.263977Z","iopub.execute_input":"2024-06-29T19:50:12.264377Z","iopub.status.idle":"2024-06-29T19:50:12.287337Z","shell.execute_reply.started":"2024-06-29T19:50:12.264345Z","shell.execute_reply":"2024-06-29T19:50:12.286104Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"For PPO Model using PEFT\nTrainable Parameters: 769\nTotal Parameters: 249348097\nPercentage of Trainable Parameters: 0.00030840419848882983\n","output_type":"stream"}]},{"cell_type":"code","source":"def collator(data):\n    return {\"input_ids\": torch.stack([torch.tensor(item[\"input_ids\"]) for item in data])}\ndef tokenize_function(examples):\n    pps = [\"Summarize the conversation\\n\" + dialogue for dialogue in examples[\"dialogue\"]]\n    return tk(pps, truncation=True, padding=\"max_length\", max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:51:04.302611Z","iopub.execute_input":"2024-06-29T19:51:04.302950Z","iopub.status.idle":"2024-06-29T19:51:04.309181Z","shell.execute_reply.started":"2024-06-29T19:51:04.302926Z","shell.execute_reply":"2024-06-29T19:51:04.308175Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"data = Dataset.from_pandas(train_df)\nfrom datasets import Dataset\ntkdt = data.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:51:04.985282Z","iopub.execute_input":"2024-06-29T19:51:04.985678Z","iopub.status.idle":"2024-06-29T19:51:11.391240Z","shell.execute_reply.started":"2024-06-29T19:51:04.985648Z","shell.execute_reply":"2024-06-29T19:51:11.390177Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe40a12e92d54925bab9651853fe03a3"}},"metadata":{}}]},{"cell_type":"markdown","source":"### TRAINING PROCESS BEGINS","metadata":{}},{"cell_type":"code","source":"from trl import PPOConfig, PPOTrainer\nconfig = PPOConfig(\n    model_name='google/flan-t5-base',\n    learning_rate=3e-5,        \n    ppo_epochs=10,               \n    mini_batch_size=16,         \n    batch_size=64,                        \n    log_with=\"wandb\"\n)\n\n\nppo_trainer = PPOTrainer(\n    config=config,\n    model=ppo_model,\n    ref_model=ref_model,\n    tokenizer=tk,\n    dataset=tkdt,\n    data_collator=collator\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:51:11.890469Z","iopub.execute_input":"2024-06-29T19:51:11.890962Z","iopub.status.idle":"2024-06-29T19:51:34.060386Z","shell.execute_reply.started":"2024-06-29T19:51:11.890926Z","shell.execute_reply":"2024-06-29T19:51:34.059227Z"},"trusted":true},"execution_count":68,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:t8erpcgh) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">still-flower-3</strong> at: <a href='https://wandb.ai/lora-project/trl/runs/t8erpcgh' target=\"_blank\">https://wandb.ai/lora-project/trl/runs/t8erpcgh</a><br/> View project at: <a href='https://wandb.ai/lora-project/trl' target=\"_blank\">https://wandb.ai/lora-project/trl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240629_193757-t8erpcgh/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:t8erpcgh). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240629_195111-ral86cik</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lora-project/trl/runs/ral86cik' target=\"_blank\">resilient-lion-4</a></strong> to <a href='https://wandb.ai/lora-project/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lora-project/trl' target=\"_blank\">https://wandb.ai/lora-project/trl</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lora-project/trl/runs/ral86cik' target=\"_blank\">https://wandb.ai/lora-project/trl/runs/ral86cik</a>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:51:34.062304Z","iopub.execute_input":"2024-06-29T19:51:34.062646Z","iopub.status.idle":"2024-06-29T19:51:34.068488Z","shell.execute_reply.started":"2024-06-29T19:51:34.062618Z","shell.execute_reply":"2024-06-29T19:51:34.067230Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"from trl.core import LengthSampler\nols = LengthSampler(100, 400)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:51:34.069788Z","iopub.execute_input":"2024-06-29T19:51:34.070089Z","iopub.status.idle":"2024-06-29T19:51:34.077385Z","shell.execute_reply.started":"2024-06-29T19:51:34.070062Z","shell.execute_reply":"2024-06-29T19:51:34.076533Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"Smp = pipeline(\n    \"sentiment-analysis\", \n    model=\"facebook/roberta-hate-speech-dynabench-r4-target\", \n    framework=\"pt\",\n    device=0 if torch.cuda.is_available() else \"cpu\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:51:34.079693Z","iopub.execute_input":"2024-06-29T19:51:34.079978Z","iopub.status.idle":"2024-06-29T19:51:35.830216Z","shell.execute_reply.started":"2024-06-29T19:51:34.079953Z","shell.execute_reply":"2024-06-29T19:51:35.828851Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"generation_kwargs = {\n    \"min_length\": 10,     \n    \"top_k\": 10,           \n    \"top_p\": 0.92,         \n    \"do_sample\": True      }","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:51:35.831678Z","iopub.execute_input":"2024-06-29T19:51:35.832023Z","iopub.status.idle":"2024-06-29T19:51:35.837898Z","shell.execute_reply.started":"2024-06-29T19:51:35.831994Z","shell.execute_reply":"2024-06-29T19:51:35.836750Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"reward_kwargs = {\n    \"top_k\": None,                  \n    \"function_to_apply\": \"none\",    \n    \"batch_size\": 16               \n}","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:51:35.839172Z","iopub.execute_input":"2024-06-29T19:51:35.839561Z","iopub.status.idle":"2024-06-29T19:51:35.846979Z","shell.execute_reply.started":"2024-06-29T19:51:35.839500Z","shell.execute_reply":"2024-06-29T19:51:35.845747Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"### TRAINING","metadata":{}},{"cell_type":"code","source":"max_s = 500\nnhi =0\nfor s, b in tqdm(enumerate(ppo_trainer.dataloader)):\n    if s >= max_s:\n        break\n    pts = [pt for pt in b[\"input_ids\"]]\n    st = []  \n    for pt in pts:\n        max_nt = ols()  \n        generation_kwargs[\"max_new_tokens\"] = max_nt\n        sm = ppo_trainer.generate(pt, **generation_kwargs) \n        st.append(sm.squeeze()[-max_nt:])\n    b[\"response\"] = [tk.decode(r.squeeze()) for r in st]\n    qrp = [tk.decode(q) + r for q, r in zip(b[\"input_ids\"], b[\"response\"])]  \n    qrp= [seq[:512] for seq in qrp]\n    rws = Smp(qrp, **reward_kwargs)  \n    rt = [torch.tensor(rw[nhi][\"score\"]) for rw in rws]\n    bc = {\"query\": [tk.decode(q) for q in b[\"input_ids\"]], \"response\": b[\"response\"]}\n    sts = ppo_trainer.step(pts, st, rt) \n    ppo_trainer.log_stats(sts, bc, rt)\n    wandb.log({\n        \"objective/kl\": sts[\"objective/kl\"],\n        \"ppo/returns/mean\": sts[\"ppo/returns/mean\"],\n        \"ppo/policy/advantages_mean\": sts[\"ppo/policy/advantages_mean\"],\n        \"step\": s\n    })\n\n    print(f'objective/kl: {sts[\"objective/kl\"]}')\n    print(f'ppo/returns/mean: {sts[\"ppo/returns/mean\"]}')\n    print(f'ppo/policy/advantages_mean: {sts[\"ppo/policy/advantages_mean\"]}')\n    print('-' * 100)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T20:03:58.733158Z","iopub.execute_input":"2024-06-29T20:03:58.734011Z","iopub.status.idle":"2024-06-30T01:37:30.333393Z","shell.execute_reply.started":"2024-06-29T20:03:58.733977Z","shell.execute_reply":"2024-06-30T01:37:30.332054Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"1it [01:45, 105.31s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.1929931640625\nppo/returns/mean: [-1.248247]\nppo/policy/advantages_mean: [7.208205e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"2it [03:19, 98.80s/it] ","output_type":"stream"},{"name":"stdout","text":"objective/kl: 39.26350402832031\nppo/returns/mean: [-1.0034271]\nppo/policy/advantages_mean: [8.865982e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"3it [05:16, 107.28s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 51.50603485107422\nppo/returns/mean: [-1.4902967]\nppo/policy/advantages_mean: [3.0859635e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"4it [07:01, 106.06s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.66033935546875\nppo/returns/mean: [-1.3579469]\nppo/policy/advantages_mean: [4.960833e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"5it [08:41, 104.17s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.70137023925781\nppo/returns/mean: [-1.2475946]\nppo/policy/advantages_mean: [3.217474e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"6it [10:20, 102.25s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.576847076416016\nppo/returns/mean: [-1.4231033]\nppo/policy/advantages_mean: [-2.882819e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"7it [11:59, 101.26s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.33234786987305\nppo/returns/mean: [-1.240242]\nppo/policy/advantages_mean: [1.6384043e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n8it [13:43, 101.97s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.80738067626953\nppo/returns/mean: [-1.5329615]\nppo/policy/advantages_mean: [1.6272498e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"9it [15:23, 101.53s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.212059020996094\nppo/returns/mean: [-1.4953781]\nppo/policy/advantages_mean: [-2.2495505e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"10it [17:14, 104.42s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.51875686645508\nppo/returns/mean: [-1.6107887]\nppo/policy/advantages_mean: [-3.6803045e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"11it [19:00, 104.99s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.961605072021484\nppo/returns/mean: [-1.7611862]\nppo/policy/advantages_mean: [-4.146188e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"12it [20:40, 103.30s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.417964935302734\nppo/returns/mean: [-1.4899721]\nppo/policy/advantages_mean: [1.3271346e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"13it [22:20, 102.49s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.92052459716797\nppo/returns/mean: [-1.7672482]\nppo/policy/advantages_mean: [-1.2232249e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"14it [24:08, 103.97s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 48.478553771972656\nppo/returns/mean: [-1.9703087]\nppo/policy/advantages_mean: [7.771752e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"15it [26:02, 107.12s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 49.78874588012695\nppo/returns/mean: [-1.978937]\nppo/policy/advantages_mean: [1.3276827e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"16it [27:50, 107.16s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.095638275146484\nppo/returns/mean: [-1.987593]\nppo/policy/advantages_mean: [-1.6726279e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"17it [29:25, 103.55s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.9332275390625\nppo/returns/mean: [-1.8892425]\nppo/policy/advantages_mean: [-1.4109788e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"18it [31:02, 101.73s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.40123748779297\nppo/returns/mean: [-1.694018]\nppo/policy/advantages_mean: [1.1600133e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"19it [32:43, 101.59s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.614280700683594\nppo/returns/mean: [-2.0389097]\nppo/policy/advantages_mean: [-5.5520637e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"20it [34:25, 101.54s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.4090576171875\nppo/returns/mean: [-1.9037716]\nppo/policy/advantages_mean: [-2.9198584e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"21it [36:04, 100.91s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.35310363769531\nppo/returns/mean: [-1.8486302]\nppo/policy/advantages_mean: [-8.6724315e-11]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"22it [37:50, 102.21s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.2106819152832\nppo/returns/mean: [-2.0179617]\nppo/policy/advantages_mean: [-9.935749e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"23it [39:36, 103.59s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.62041473388672\nppo/returns/mean: [-2.1272085]\nppo/policy/advantages_mean: [-1.3877806e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"24it [41:13, 101.37s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.2906379699707\nppo/returns/mean: [-1.8970855]\nppo/policy/advantages_mean: [8.320081e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"25it [42:57, 102.26s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.76048278808594\nppo/returns/mean: [-2.2127838]\nppo/policy/advantages_mean: [1.0304689e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"26it [44:46, 104.19s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.23925018310547\nppo/returns/mean: [-1.9248631]\nppo/policy/advantages_mean: [-2.5311784e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"27it [46:30, 104.20s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.078155517578125\nppo/returns/mean: [-2.037887]\nppo/policy/advantages_mean: [2.3210265e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"28it [48:10, 103.04s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.365352630615234\nppo/returns/mean: [-2.069674]\nppo/policy/advantages_mean: [-7.4712103e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"29it [49:52, 102.79s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.32463073730469\nppo/returns/mean: [-1.9984677]\nppo/policy/advantages_mean: [-2.1291726e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"30it [51:34, 102.49s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.18092346191406\nppo/returns/mean: [-2.279287]\nppo/policy/advantages_mean: [-6.6478284e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"31it [53:19, 103.20s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.529048919677734\nppo/returns/mean: [-2.234198]\nppo/policy/advantages_mean: [-1.5952527e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"32it [54:58, 102.03s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.63550567626953\nppo/returns/mean: [-1.9422349]\nppo/policy/advantages_mean: [-1.4417483e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"33it [56:36, 100.70s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.38597106933594\nppo/returns/mean: [-2.095506]\nppo/policy/advantages_mean: [-1.6789151e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"34it [58:19, 101.41s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.2947998046875\nppo/returns/mean: [-2.158015]\nppo/policy/advantages_mean: [-1.0769312e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"35it [1:00:12, 104.90s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.489471435546875\nppo/returns/mean: [-2.3327973]\nppo/policy/advantages_mean: [1.656281e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"36it [1:01:54, 104.04s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.74046325683594\nppo/returns/mean: [-1.9931576]\nppo/policy/advantages_mean: [2.0028006e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"37it [1:03:41, 104.81s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.39963150024414\nppo/returns/mean: [-2.181116]\nppo/policy/advantages_mean: [-4.689315e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"38it [1:05:26, 104.88s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.0620231628418\nppo/returns/mean: [-2.4707272]\nppo/policy/advantages_mean: [-1.8716838e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"39it [1:07:04, 102.84s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.29264831542969\nppo/returns/mean: [-2.130905]\nppo/policy/advantages_mean: [-5.5026583e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"40it [1:08:45, 102.37s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.28458786010742\nppo/returns/mean: [-2.1874802]\nppo/policy/advantages_mean: [-2.6944713e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"41it [1:10:23, 100.95s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.27394104003906\nppo/returns/mean: [-2.3206494]\nppo/policy/advantages_mean: [3.360183e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"42it [1:11:58, 99.33s/it] ","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.69149398803711\nppo/returns/mean: [-2.1594913]\nppo/policy/advantages_mean: [-3.849574e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"43it [1:13:37, 99.10s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.490501403808594\nppo/returns/mean: [-2.1069314]\nppo/policy/advantages_mean: [2.4068354e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"44it [1:15:14, 98.39s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.80034637451172\nppo/returns/mean: [-2.054736]\nppo/policy/advantages_mean: [-1.7933939e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"45it [1:16:44, 96.13s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.631736755371094\nppo/returns/mean: [-2.1521602]\nppo/policy/advantages_mean: [-1.4612473e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"46it [1:18:20, 96.05s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.67881393432617\nppo/returns/mean: [-2.2012594]\nppo/policy/advantages_mean: [-3.6416254e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"47it [1:19:58, 96.70s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.280704498291016\nppo/returns/mean: [-2.2653005]\nppo/policy/advantages_mean: [-2.1709585e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"48it [1:21:39, 97.90s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.75255584716797\nppo/returns/mean: [-2.6017418]\nppo/policy/advantages_mean: [-4.3481374e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"49it [1:23:12, 96.51s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.67500305175781\nppo/returns/mean: [-2.3850048]\nppo/policy/advantages_mean: [-1.4412452e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"50it [1:24:52, 97.40s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.80695343017578\nppo/returns/mean: [-2.390937]\nppo/policy/advantages_mean: [-3.2752853e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"51it [1:26:32, 98.15s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.473968505859375\nppo/returns/mean: [-2.3775978]\nppo/policy/advantages_mean: [-3.9487688e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"52it [1:28:19, 100.76s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.29935073852539\nppo/returns/mean: [-2.4455736]\nppo/policy/advantages_mean: [2.068086e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -104.41 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n  warnings.warn(\n53it [1:29:59, 100.58s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: -104.41354370117188\nppo/returns/mean: [9.817229]\nppo/policy/advantages_mean: [6.2347283e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"54it [1:31:42, 101.44s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.90965270996094\nppo/returns/mean: [-2.630547]\nppo/policy/advantages_mean: [-2.3061715e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"55it [1:33:35, 104.72s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 49.715248107910156\nppo/returns/mean: [-2.6821423]\nppo/policy/advantages_mean: [-1.211939e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"56it [1:35:10, 102.01s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.9543571472168\nppo/returns/mean: [-2.390391]\nppo/policy/advantages_mean: [-1.1923367e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"57it [1:36:52, 102.00s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.1628303527832\nppo/returns/mean: [-2.462496]\nppo/policy/advantages_mean: [1.6087335e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"58it [1:38:34, 102.05s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.64837646484375\nppo/returns/mean: [-2.5986454]\nppo/policy/advantages_mean: [1.9864637e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"59it [1:40:18, 102.45s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.397647857666016\nppo/returns/mean: [-2.5896862]\nppo/policy/advantages_mean: [2.05982e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"60it [1:41:53, 100.30s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.087074279785156\nppo/returns/mean: [-2.4441612]\nppo/policy/advantages_mean: [5.062697e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"61it [1:43:45, 103.82s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 49.69673156738281\nppo/returns/mean: [-2.8143117]\nppo/policy/advantages_mean: [2.2451303e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"62it [1:45:25, 102.59s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.33798599243164\nppo/returns/mean: [-2.4134533]\nppo/policy/advantages_mean: [2.6346267e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"63it [1:47:25, 107.78s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 51.246280670166016\nppo/returns/mean: [-2.8133476]\nppo/policy/advantages_mean: [2.4957645e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"64it [1:49:05, 105.53s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.021087646484375\nppo/returns/mean: [-2.464292]\nppo/policy/advantages_mean: [-7.085085e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"65it [1:50:49, 105.05s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.23139190673828\nppo/returns/mean: [-2.5714738]\nppo/policy/advantages_mean: [-3.5659973e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"66it [1:52:37, 105.98s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.43136978149414\nppo/returns/mean: [-2.5946522]\nppo/policy/advantages_mean: [-2.6673501e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"67it [1:54:23, 105.96s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.51146697998047\nppo/returns/mean: [-2.5986526]\nppo/policy/advantages_mean: [1.4300773e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"68it [1:56:04, 104.40s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.312355041503906\nppo/returns/mean: [-2.397329]\nppo/policy/advantages_mean: [-2.6935634e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"69it [1:57:44, 103.19s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.946258544921875\nppo/returns/mean: [-2.574752]\nppo/policy/advantages_mean: [1.6150455e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"70it [1:59:25, 102.58s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.54866027832031\nppo/returns/mean: [-2.2165558]\nppo/policy/advantages_mean: [-9.789842e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"71it [2:01:07, 102.39s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.947513580322266\nppo/returns/mean: [-2.553075]\nppo/policy/advantages_mean: [1.6970019e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"72it [2:02:57, 104.54s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 49.04149627685547\nppo/returns/mean: [-2.786923]\nppo/policy/advantages_mean: [-3.0397598e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"73it [2:04:35, 102.58s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.99378967285156\nppo/returns/mean: [-2.4473312]\nppo/policy/advantages_mean: [-4.0435144e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"74it [2:06:13, 101.15s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.62212371826172\nppo/returns/mean: [-2.5796573]\nppo/policy/advantages_mean: [1.046996e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"75it [2:07:51, 100.38s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.61351013183594\nppo/returns/mean: [-2.5437534]\nppo/policy/advantages_mean: [7.057879e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"76it [2:09:28, 99.18s/it] ","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.38911437988281\nppo/returns/mean: [-2.4199035]\nppo/policy/advantages_mean: [-2.42124e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"77it [2:11:11, 100.41s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.08612823486328\nppo/returns/mean: [-2.3701942]\nppo/policy/advantages_mean: [5.9445693e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"78it [2:12:57, 102.09s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.60987854003906\nppo/returns/mean: [-2.804636]\nppo/policy/advantages_mean: [1.3553214e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"79it [2:14:39, 102.06s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.41096496582031\nppo/returns/mean: [-2.6854422]\nppo/policy/advantages_mean: [-2.7449303e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"80it [2:16:32, 105.38s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 50.63219451904297\nppo/returns/mean: [-2.8166158]\nppo/policy/advantages_mean: [-1.0832427e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"81it [2:18:12, 103.90s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.5780143737793\nppo/returns/mean: [-2.5154712]\nppo/policy/advantages_mean: [1.3076312e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"82it [2:19:48, 101.48s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.1947021484375\nppo/returns/mean: [-2.552182]\nppo/policy/advantages_mean: [-3.4717844e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"83it [2:21:24, 99.84s/it] ","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.59491729736328\nppo/returns/mean: [-2.4613476]\nppo/policy/advantages_mean: [1.3133845e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"84it [2:23:15, 102.96s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.392799377441406\nppo/returns/mean: [-2.6842172]\nppo/policy/advantages_mean: [2.6351377e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"85it [2:25:03, 104.74s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 48.25853729248047\nppo/returns/mean: [-2.8328178]\nppo/policy/advantages_mean: [-1.6457753e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"86it [2:26:44, 103.59s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.190460205078125\nppo/returns/mean: [-2.834625]\nppo/policy/advantages_mean: [-1.4651155e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"87it [2:28:26, 102.95s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.79917907714844\nppo/returns/mean: [-2.5219598]\nppo/policy/advantages_mean: [-1.6054253e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"88it [2:30:04, 101.40s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.24867248535156\nppo/returns/mean: [-2.6517632]\nppo/policy/advantages_mean: [-1.0178693e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -102.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n  warnings.warn(\n89it [2:31:48, 102.17s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: -102.65316009521484\nppo/returns/mean: [9.446612]\nppo/policy/advantages_mean: [3.2997233e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"90it [2:33:37, 104.49s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.81911087036133\nppo/returns/mean: [-2.6707723]\nppo/policy/advantages_mean: [-2.0195159e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"91it [2:35:26, 105.61s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.75248718261719\nppo/returns/mean: [-2.8755238]\nppo/policy/advantages_mean: [5.032709e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"92it [2:37:09, 104.94s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.51445770263672\nppo/returns/mean: [-2.9588006]\nppo/policy/advantages_mean: [-3.7900256e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -118.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n  warnings.warn(\n93it [2:38:56, 105.68s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: -118.77383422851562\nppo/returns/mean: [10.3399725]\nppo/policy/advantages_mean: [6.332144e-11]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"94it [2:40:43, 105.98s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.18824005126953\nppo/returns/mean: [-2.8501883]\nppo/policy/advantages_mean: [-2.049434e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"95it [2:42:19, 102.90s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.527191162109375\nppo/returns/mean: [-2.4144173]\nppo/policy/advantages_mean: [-2.2775755e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"96it [2:43:57, 101.41s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.249267578125\nppo/returns/mean: [-2.6668396]\nppo/policy/advantages_mean: [-1.7152487e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"97it [2:45:35, 100.34s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.41973114013672\nppo/returns/mean: [-2.7589278]\nppo/policy/advantages_mean: [1.694158e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"98it [2:47:23, 102.82s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.97879409790039\nppo/returns/mean: [-2.8174574]\nppo/policy/advantages_mean: [-1.0727217e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"99it [2:49:00, 100.99s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.169918060302734\nppo/returns/mean: [-2.6254797]\nppo/policy/advantages_mean: [-1.0636065e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"100it [2:50:33, 98.52s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 39.443782806396484\nppo/returns/mean: [-2.5396729]\nppo/policy/advantages_mean: [-1.4809606e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"101it [2:52:13, 99.19s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.263938903808594\nppo/returns/mean: [-2.8852828]\nppo/policy/advantages_mean: [2.644011e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"102it [2:53:52, 98.96s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.471153259277344\nppo/returns/mean: [-2.5883965]\nppo/policy/advantages_mean: [9.89319e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"103it [2:55:37, 100.71s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.73162078857422\nppo/returns/mean: [-2.6778848]\nppo/policy/advantages_mean: [1.2161037e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"104it [2:57:23, 102.42s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 48.343536376953125\nppo/returns/mean: [-2.9902718]\nppo/policy/advantages_mean: [8.534157e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"105it [2:59:05, 102.12s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.073524475097656\nppo/returns/mean: [-2.8423202]\nppo/policy/advantages_mean: [-2.7296883e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"106it [3:00:45, 101.61s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.49288558959961\nppo/returns/mean: [-2.7765691]\nppo/policy/advantages_mean: [-3.6901207e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"107it [3:02:25, 101.13s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.70365905761719\nppo/returns/mean: [-2.8169782]\nppo/policy/advantages_mean: [2.1517863e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"108it [3:04:08, 101.85s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.704002380371094\nppo/returns/mean: [-2.626162]\nppo/policy/advantages_mean: [8.920873e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"109it [3:05:44, 99.85s/it] ","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.57978820800781\nppo/returns/mean: [-2.8520947]\nppo/policy/advantages_mean: [4.1788264e-11]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"110it [3:07:32, 102.44s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.22392272949219\nppo/returns/mean: [-3.0572793]\nppo/policy/advantages_mean: [1.0139017e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"111it [3:09:21, 104.23s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.272422790527344\nppo/returns/mean: [-2.9393349]\nppo/policy/advantages_mean: [-4.99555e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"112it [3:11:07, 104.77s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.78474044799805\nppo/returns/mean: [-3.0116603]\nppo/policy/advantages_mean: [1.1670618e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"113it [3:12:52, 104.82s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.258819580078125\nppo/returns/mean: [-2.9916835]\nppo/policy/advantages_mean: [2.5855844e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"114it [3:14:37, 104.96s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.531341552734375\nppo/returns/mean: [-2.850481]\nppo/policy/advantages_mean: [1.9095892e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"115it [3:16:14, 102.50s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.986534118652344\nppo/returns/mean: [-2.653235]\nppo/policy/advantages_mean: [1.2867162e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"116it [3:17:51, 101.10s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.47972106933594\nppo/returns/mean: [-2.7102287]\nppo/policy/advantages_mean: [2.6756705e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"117it [3:19:39, 103.19s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.40849304199219\nppo/returns/mean: [-2.9151971]\nppo/policy/advantages_mean: [5.84341e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"118it [3:21:21, 102.75s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.75963592529297\nppo/returns/mean: [-2.8874452]\nppo/policy/advantages_mean: [-9.074667e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"119it [3:23:11, 104.82s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.9398307800293\nppo/returns/mean: [-2.87246]\nppo/policy/advantages_mean: [3.3318448e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"120it [3:24:55, 104.75s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.80064392089844\nppo/returns/mean: [-2.879821]\nppo/policy/advantages_mean: [-8.577059e-11]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -136.29 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n  warnings.warn(\n121it [3:26:52, 108.25s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: -136.28623962402344\nppo/returns/mean: [12.5494585]\nppo/policy/advantages_mean: [2.576748e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"122it [3:28:34, 106.44s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.40625762939453\nppo/returns/mean: [-3.0977862]\nppo/policy/advantages_mean: [1.4338671e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"123it [3:30:26, 107.99s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 48.19804382324219\nppo/returns/mean: [-3.256375]\nppo/policy/advantages_mean: [3.4146998e-11]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"124it [3:32:08, 106.25s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.032073974609375\nppo/returns/mean: [-3.0048885]\nppo/policy/advantages_mean: [7.937618e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"125it [3:33:50, 105.06s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.41069793701172\nppo/returns/mean: [-2.981595]\nppo/policy/advantages_mean: [-1.1261397e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"126it [3:35:38, 105.92s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.83472442626953\nppo/returns/mean: [-2.988524]\nppo/policy/advantages_mean: [8.6808666e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"127it [3:37:20, 104.60s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.9361572265625\nppo/returns/mean: [-2.7442234]\nppo/policy/advantages_mean: [-5.0404375e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"128it [3:39:03, 104.27s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.35873031616211\nppo/returns/mean: [-2.9308314]\nppo/policy/advantages_mean: [1.8967495e-12]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"129it [3:40:47, 104.29s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.07502746582031\nppo/returns/mean: [-3.1041982]\nppo/policy/advantages_mean: [4.1406047e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"130it [3:42:33, 104.56s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.822364807128906\nppo/returns/mean: [-3.0297081]\nppo/policy/advantages_mean: [1.52365e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"131it [3:44:13, 103.42s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.41239929199219\nppo/returns/mean: [-2.9588041]\nppo/policy/advantages_mean: [7.7114154e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"132it [3:46:00, 104.41s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.875946044921875\nppo/returns/mean: [-2.9056592]\nppo/policy/advantages_mean: [-1.0179713e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"133it [3:47:45, 104.51s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.3665657043457\nppo/returns/mean: [-2.8222318]\nppo/policy/advantages_mean: [1.7055568e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"134it [3:49:30, 104.66s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 50.05619430541992\nppo/returns/mean: [-3.4865243]\nppo/policy/advantages_mean: [-8.7659674e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"135it [3:51:14, 104.58s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.029396057128906\nppo/returns/mean: [-2.9678848]\nppo/policy/advantages_mean: [-3.871492e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"136it [3:52:52, 102.53s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.01642608642578\nppo/returns/mean: [-3.1929817]\nppo/policy/advantages_mean: [1.9000475e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"137it [3:54:31, 101.38s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.59152603149414\nppo/returns/mean: [-3.0319147]\nppo/policy/advantages_mean: [-2.6002134e-11]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"138it [3:56:11, 101.04s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.49834060668945\nppo/returns/mean: [-2.909436]\nppo/policy/advantages_mean: [4.6646037e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"139it [3:57:49, 100.21s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.741912841796875\nppo/returns/mean: [-3.0300922]\nppo/policy/advantages_mean: [-7.3627826e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"140it [3:59:34, 101.71s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.536766052246094\nppo/returns/mean: [-3.1688695]\nppo/policy/advantages_mean: [-1.8796058e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"141it [4:01:19, 102.44s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.570899963378906\nppo/returns/mean: [-3.1112509]\nppo/policy/advantages_mean: [-9.330231e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"142it [4:03:03, 103.13s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.07859802246094\nppo/returns/mean: [-2.9840817]\nppo/policy/advantages_mean: [5.6227254e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"143it [4:04:47, 103.43s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.36376953125\nppo/returns/mean: [-3.2012544]\nppo/policy/advantages_mean: [1.761661e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"144it [4:06:30, 103.29s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.73997497558594\nppo/returns/mean: [-3.1530282]\nppo/policy/advantages_mean: [8.6004004e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -171.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n  warnings.warn(\n145it [4:08:25, 106.63s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: -171.48281860351562\nppo/returns/mean: [15.429839]\nppo/policy/advantages_mean: [-1.3339841e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"146it [4:10:11, 106.64s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.50822448730469\nppo/returns/mean: [-3.055561]\nppo/policy/advantages_mean: [-7.460823e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"147it [4:11:54, 105.50s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.803768157958984\nppo/returns/mean: [-2.8619926]\nppo/policy/advantages_mean: [1.9445681e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"148it [4:13:45, 106.93s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.05823516845703\nppo/returns/mean: [-3.2512949]\nppo/policy/advantages_mean: [-1.2243037e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"149it [4:15:28, 105.87s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.24188232421875\nppo/returns/mean: [-2.981425]\nppo/policy/advantages_mean: [-1.8411894e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"150it [4:17:20, 107.65s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.162322998046875\nppo/returns/mean: [-3.2139027]\nppo/policy/advantages_mean: [-1.0164369e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"151it [4:19:10, 108.44s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 50.93034744262695\nppo/returns/mean: [-3.691998]\nppo/policy/advantages_mean: [3.980184e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"152it [4:20:55, 107.40s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.28141784667969\nppo/returns/mean: [-3.1331918]\nppo/policy/advantages_mean: [-1.0784114e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"153it [4:22:45, 108.24s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.99629592895508\nppo/returns/mean: [-3.2434278]\nppo/policy/advantages_mean: [1.0452963e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"154it [4:24:22, 104.92s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.179378509521484\nppo/returns/mean: [-3.0730922]\nppo/policy/advantages_mean: [-2.1434863e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"155it [4:26:01, 103.01s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.579063415527344\nppo/returns/mean: [-3.189268]\nppo/policy/advantages_mean: [-1.6750203e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"156it [4:27:40, 101.79s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.90106201171875\nppo/returns/mean: [-3.0564396]\nppo/policy/advantages_mean: [1.1085496e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"157it [4:29:16, 99.95s/it] ","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.287437438964844\nppo/returns/mean: [-2.9334137]\nppo/policy/advantages_mean: [2.1430193e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"158it [4:30:48, 97.81s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 39.80051040649414\nppo/returns/mean: [-2.969368]\nppo/policy/advantages_mean: [8.339642e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"159it [4:32:27, 98.18s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.524658203125\nppo/returns/mean: [-2.991745]\nppo/policy/advantages_mean: [-6.581244e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"160it [4:34:15, 101.14s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 48.282867431640625\nppo/returns/mean: [-3.6273887]\nppo/policy/advantages_mean: [2.5250297e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"161it [4:35:52, 99.63s/it] ","output_type":"stream"},{"name":"stdout","text":"objective/kl: 40.92456817626953\nppo/returns/mean: [-2.895635]\nppo/policy/advantages_mean: [-7.489822e-11]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"162it [4:37:43, 103.21s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.09370422363281\nppo/returns/mean: [-3.320126]\nppo/policy/advantages_mean: [-9.908728e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"163it [4:39:21, 101.59s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.9051513671875\nppo/returns/mean: [-3.2035015]\nppo/policy/advantages_mean: [1.722686e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"164it [4:41:00, 100.69s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.455169677734375\nppo/returns/mean: [-3.1843693]\nppo/policy/advantages_mean: [-2.7728486e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"165it [4:42:50, 103.59s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 50.614479064941406\nppo/returns/mean: [-3.57315]\nppo/policy/advantages_mean: [-1.1181953e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"166it [4:44:37, 104.77s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.571006774902344\nppo/returns/mean: [-3.4372177]\nppo/policy/advantages_mean: [-6.203639e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"167it [4:46:19, 103.82s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.25616455078125\nppo/returns/mean: [-3.163305]\nppo/policy/advantages_mean: [2.5249487e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"168it [4:48:02, 103.56s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.044795989990234\nppo/returns/mean: [-3.1372373]\nppo/policy/advantages_mean: [-1.7604114e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"169it [4:49:45, 103.49s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.42433547973633\nppo/returns/mean: [-2.9798725]\nppo/policy/advantages_mean: [-6.9216155e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"170it [4:51:34, 105.05s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.201744079589844\nppo/returns/mean: [-3.524407]\nppo/policy/advantages_mean: [-3.3226293e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"171it [4:53:12, 102.96s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.189910888671875\nppo/returns/mean: [-3.0281913]\nppo/policy/advantages_mean: [-8.398452e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"172it [4:54:53, 102.47s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.84884262084961\nppo/returns/mean: [-3.3794286]\nppo/policy/advantages_mean: [-9.175943e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"173it [4:56:41, 104.13s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.5833740234375\nppo/returns/mean: [-3.5119827]\nppo/policy/advantages_mean: [-1.8237881e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"174it [4:58:24, 103.72s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.86591339111328\nppo/returns/mean: [-3.4257946]\nppo/policy/advantages_mean: [-2.1652198e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"175it [5:00:09, 104.07s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.15773010253906\nppo/returns/mean: [-3.2334893]\nppo/policy/advantages_mean: [3.914523e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"176it [5:01:57, 105.10s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 48.252620697021484\nppo/returns/mean: [-3.6970768]\nppo/policy/advantages_mean: [4.0234953e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"177it [5:03:42, 105.20s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.23786163330078\nppo/returns/mean: [-3.3664472]\nppo/policy/advantages_mean: [-5.030373e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"178it [5:05:23, 103.92s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.86602020263672\nppo/returns/mean: [-3.184194]\nppo/policy/advantages_mean: [-8.025673e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"179it [5:07:01, 102.06s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.1179313659668\nppo/returns/mean: [-3.2078903]\nppo/policy/advantages_mean: [1.4152943e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"180it [5:08:53, 105.01s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 47.616233825683594\nppo/returns/mean: [-3.512989]\nppo/policy/advantages_mean: [2.5380598e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"181it [5:10:35, 104.28s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.84468078613281\nppo/returns/mean: [-3.1108868]\nppo/policy/advantages_mean: [8.5101787e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"182it [5:12:23, 105.39s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 46.38486862182617\nppo/returns/mean: [-3.4960563]\nppo/policy/advantages_mean: [2.4298303e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"183it [5:14:05, 104.33s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.82567596435547\nppo/returns/mean: [-3.1982312]\nppo/policy/advantages_mean: [4.2598566e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"184it [5:15:54, 105.83s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.56631851196289\nppo/returns/mean: [-3.35087]\nppo/policy/advantages_mean: [4.4072265e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"185it [5:17:44, 106.89s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.06428146362305\nppo/returns/mean: [-3.3667164]\nppo/policy/advantages_mean: [1.671041e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"186it [5:19:28, 106.24s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 42.58354949951172\nppo/returns/mean: [-3.2249856]\nppo/policy/advantages_mean: [2.1494302e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"187it [5:21:09, 104.58s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.758056640625\nppo/returns/mean: [-3.4660728]\nppo/policy/advantages_mean: [-9.818631e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"188it [5:22:51, 103.87s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 41.063560485839844\nppo/returns/mean: [-3.1448085]\nppo/policy/advantages_mean: [4.1962146e-11]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"189it [5:24:33, 103.09s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.198570251464844\nppo/returns/mean: [-3.5611012]\nppo/policy/advantages_mean: [-3.3925165e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -85.94 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n  warnings.warn(\n190it [5:26:22, 105.05s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: -85.93911743164062\nppo/returns/mean: [8.716857]\nppo/policy/advantages_mean: [-2.9541671e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"191it [5:28:00, 102.95s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 43.1212272644043\nppo/returns/mean: [-3.2670746]\nppo/policy/advantages_mean: [-1.7432672e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1091: UserWarning: KL divergence is starting to become negative: -91.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n  warnings.warn(\n192it [5:29:50, 104.98s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: -91.50237274169922\nppo/returns/mean: [9.064597]\nppo/policy/advantages_mean: [1.7336284e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"193it [5:31:39, 106.15s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 44.98982620239258\nppo/returns/mean: [-3.357119]\nppo/policy/advantages_mean: [1.3133832e-10]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"194it [5:33:24, 103.11s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 45.2420654296875\nppo/returns/mean: [-3.4168382]\nppo/policy/advantages_mean: [1.868697e-09]\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='56.192 MB of 56.192 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>env/reward_mean</td><td>▄▇▆▃▄▇▅▆▅▆▃▄▂▇█▅▇▇▆▇▅▅▄▅▄▂▆▄▅▅▇▄▃▁▆▅▅█▆▆</td></tr><tr><td>env/reward_std</td><td>▆▇▇▆█▅█▆██▇▆▆▆▅▆▄▆▇▅▇▁▇▅▄▇▆█▆▅█▆▅█▆▇▇▄▅▄</td></tr><tr><td>objective/entropy</td><td>▂▂█▄▃▇▅▄▅▃▅▃▃▅▅▄▆█▆▇▂▆▆▆▃▄▆▅▄▃▆▁▂▅▆▃▄▅▄▄</td></tr><tr><td>objective/kl</td><td>▁▅▆▅▄▄▅▃▃▃▃▃▂▅▂▄█▅▄▅▁▇▅▄▃▃▆█▃▄▅▂▁▃▄▆▃▄▂▅</td></tr><tr><td>objective/kl_coef</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>ppo/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/loss/policy</td><td>▃▇█▆▇▇▅▄▄█▄▅▃▇▅▄▅▁▆▆▆▃▃▂▄▃▂▅▃▃▅▆▃▄▆▆▅▃▄▂</td></tr><tr><td>ppo/loss/total</td><td>▁▃▂▂▃▂▄▂▃▃▂▄▃▄▄▄▅▄▄▅▄▅▅▅▅▄▇█▅▇▇▇▆▆▇█▇▇▆█</td></tr><tr><td>ppo/loss/value</td><td>▁▃▂▂▃▂▄▂▃▃▂▄▃▄▄▄▅▄▄▅▄▅▅▅▅▄▇█▅▇▇▇▆▆▇█▇▇▆█</td></tr><tr><td>ppo/mean_non_score_reward</td><td>▇▅▇▇▇█▅▇▇▅▆▅▅▆▇▄▅▆▆▅▄▄▅▅▅▄▃▂▄▃▄▃▂▃▃▁▂▃▃▁</td></tr><tr><td>ppo/mean_scores</td><td>▄▇▆▃▄▇▅▆▅▆▃▄▂▇█▅▇▇▆▇▅▅▄▅▄▂▆▄▅▅▇▄▃▁▆▅▅█▆▆</td></tr><tr><td>ppo/policy/advantages_mean</td><td>▆▂▁▃▂▂▄▅▅▁▅▄▆▂▄▅▄█▃▃▃▆▆▇▅▆▇▄▆▆▄▃▆▅▃▃▄▆▅▇</td></tr><tr><td>ppo/policy/approxkl</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/policy/clipfrac</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/policy/entropy</td><td>▄▂▇▃▁▅▄▅▃▄▅▅▇▂▇▆▃▆▃▇▃▅▃▆▂▄▇▃▆▁▅▂▅█▆▂▄▃▄▅</td></tr><tr><td>ppo/policy/policykl</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/returns/mean</td><td>█▇▆▅▅▅▄▅▅▅▄▄▄▄▅▄▃▃▃▃▄▂▂▃▃▂▂▁▃▂▂▂▂▂▂▁▂▁▂▁</td></tr><tr><td>ppo/returns/var</td><td>▁▂▂▂▃▃▄▃▃▄▃▅▄▅▄▅▅▅▅▅▅▅▅▆▆▅▇▇▆▇▆▇▆▆▇██▇▇█</td></tr><tr><td>ppo/std_scores</td><td>▆▇▇▆█▅█▆██▇▆▆▆▅▆▄▆▇▅▇▁▇▅▄▇▆█▆▅█▆▅█▆▇▇▄▅▄</td></tr><tr><td>ppo/time/ppo/optimizer_step</td><td>▅▆█▄▄▄▃▃▁▂▃▃▄▅▄▃▃▄▄▅▂▃▄▃▇▆▆▄▃▇▇█▃▃▃▁▄▅▅▅</td></tr><tr><td>ppo/val/clipfrac</td><td>▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█</td></tr><tr><td>ppo/val/error</td><td>▁▃▂▂▃▂▄▂▃▃▂▄▃▄▄▄▅▄▄▅▄▅▅▅▅▄▇█▅▇▇▇▆▆▇█▇▇▆▇</td></tr><tr><td>ppo/val/mean</td><td>█▇▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>ppo/val/var</td><td>▁▁▂▃▃▄▄▅▅▅▅▅▅▆▅▆▆▆▆▆▇▆▇▆▆▆▆▇▇▇▇▇▇▇█▇▇█▇█</td></tr><tr><td>ppo/val/var_explained</td><td>▁▁▁▂▄▅▄▆▆▇▇▇▇▆▇▇▆▇▇▆▇▆▇▇█▇▇▆▇▇▆▇███▇█▇▇▇</td></tr><tr><td>ppo/val/vpred</td><td>█▇▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/ppo/calc_stats</td><td>▃▄▂▃▁▃▃▂▃▁▂▄▄▃▃▂▃▃▂▄▂▃▂▄▃▄▄▃▃▂█▂▁▃▄▂▃█▃▄</td></tr><tr><td>time/ppo/compute_rewards</td><td>▅▃▃▄▅▅▆▄▆▇▁▄▅▆▂▄▃▄▄▃▄▃▅▆█▆▄▅▅▅█▅▆▄▆▃▆▅▇▅</td></tr><tr><td>time/ppo/forward_pass</td><td>▁▂▆▆▃▅▅▆▃▂▂▂▆▄▅▂█▇█▄▁▄▆▅▅▁▃▅▄▄▅▄▁▃▃▆▄▃▁▆</td></tr><tr><td>time/ppo/optimize_step</td><td>▁▂▆▆▃▅▅▆▃▂▂▂▆▄▅▂█▇█▄▁▄▆▅▅▁▃▅▄▅▅▄▁▃▃▆▄▃▁▆</td></tr><tr><td>time/ppo/total</td><td>▁▂▆▆▃▅▅▆▃▂▂▂▆▄▅▂█▇█▄▁▄▆▅▅▁▃▅▄▅▅▄▁▃▃▆▄▃▁▆</td></tr><tr><td>tokens/queries_len_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tokens/queries_len_std</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tokens/responses_len_mean</td><td>▂▃▇▆▅▇▅▄▅▃▄▂▂▆▄▄█▇▇▆▁▆▆▅▄▄▅▆▃▄▆▂▁▃▅▄▄▆▄▄</td></tr><tr><td>tokens/responses_len_std</td><td>▂▁▆█▄▅▅▄▃▁▄▂▆▆▄▂█▇▅▆▂▄▇█▆▂▄▇▄▄▅▃▂▄▃▅▄▅▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>env/reward_mean</td><td>2.78169</td></tr><tr><td>env/reward_std</td><td>0.95177</td></tr><tr><td>objective/entropy</td><td>24.75236</td></tr><tr><td>objective/kl</td><td>45.24207</td></tr><tr><td>objective/kl_coef</td><td>0.25178</td></tr><tr><td>ppo/learning_rate</td><td>3e-05</td></tr><tr><td>ppo/loss/policy</td><td>-0.0</td></tr><tr><td>ppo/loss/total</td><td>0.53832</td></tr><tr><td>ppo/loss/value</td><td>5.38322</td></tr><tr><td>ppo/mean_non_score_reward</td><td>-0.27552</td></tr><tr><td>ppo/mean_scores</td><td>2.78169</td></tr><tr><td>ppo/policy/advantages_mean</td><td>0.0</td></tr><tr><td>ppo/policy/approxkl</td><td>0.0</td></tr><tr><td>ppo/policy/clipfrac</td><td>0.0</td></tr><tr><td>ppo/policy/entropy</td><td>0.96942</td></tr><tr><td>ppo/policy/policykl</td><td>0.0</td></tr><tr><td>ppo/returns/mean</td><td>-3.41684</td></tr><tr><td>ppo/returns/var</td><td>12.04341</td></tr><tr><td>ppo/std_scores</td><td>0.95177</td></tr><tr><td>ppo/time/ppo/optimizer_step</td><td>0.00099</td></tr><tr><td>ppo/val/clipfrac</td><td>0.25197</td></tr><tr><td>ppo/val/error</td><td>10.42856</td></tr><tr><td>ppo/val/mean</td><td>-3.08006</td></tr><tr><td>ppo/val/var</td><td>1.50873</td></tr><tr><td>ppo/val/var_explained</td><td>0.13409</td></tr><tr><td>ppo/val/vpred</td><td>-3.09205</td></tr><tr><td>step</td><td>193</td></tr><tr><td>time/ppo/calc_stats</td><td>0.01237</td></tr><tr><td>time/ppo/compute_rewards</td><td>0.00982</td></tr><tr><td>time/ppo/forward_pass</td><td>4.51018</td></tr><tr><td>time/ppo/optimize_step</td><td>23.77007</td></tr><tr><td>time/ppo/total</td><td>28.34836</td></tr><tr><td>tokens/queries_len_mean</td><td>512.0</td></tr><tr><td>tokens/queries_len_std</td><td>0.0</td></tr><tr><td>tokens/responses_len_mean</td><td>43.34375</td></tr><tr><td>tokens/responses_len_std</td><td>17.98166</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">resilient-lion-4</strong> at: <a href='https://wandb.ai/lora-project/trl/runs/ral86cik' target=\"_blank\">https://wandb.ai/lora-project/trl/runs/ral86cik</a><br/> View project at: <a href='https://wandb.ai/lora-project/trl' target=\"_blank\">https://wandb.ai/lora-project/trl</a><br/>Synced 5 W&B file(s), 194 media file(s), 194 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240629_195111-ral86cik/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"### SAVING AND EVALUATING ","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nfrom IPython.display import FileLink\nsave_path = \"./final_ppo_model\"\nos.makedirs(save_path, exist_ok=True)\nppo_trainer.model.save_pretrained(save_path)\ntk.save_pretrained(save_path)\nprint(f\"Model and tokenizer saved to '{save_path}'\")\nshutil.make_archive(\"final_ppo_model\", 'zip', save_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T02:07:41.756123Z","iopub.execute_input":"2024-06-30T02:07:41.756891Z","iopub.status.idle":"2024-06-30T02:07:42.469885Z","shell.execute_reply.started":"2024-06-30T02:07:41.756856Z","shell.execute_reply":"2024-06-30T02:07:42.468876Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Model and tokenizer saved to './final_ppo_model'\n","output_type":"stream"},{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/final_ppo_model.zip'"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'final_ppo_model.zip')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T02:07:53.605070Z","iopub.execute_input":"2024-06-30T02:07:53.605455Z","iopub.status.idle":"2024-06-30T02:07:53.612667Z","shell.execute_reply.started":"2024-06-30T02:07:53.605424Z","shell.execute_reply":"2024-06-30T02:07:53.611706Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/final_ppo_model.zip","text/html":"<a href='final_ppo_model.zip' target='_blank'>final_ppo_model.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForSequenceClassification\nfrom peft import PeftModel\nmodel_path = \"./final_ppo_model\"\ntk = AutoTokenizer.from_pretrained(model_path)\nog = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\npm = PeftModel.from_pretrained(og, model_path)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npm.to(device)\ntoxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\ntox_tk = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\ntox_mdl = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\nprint(tox_mdl.config.id2label)\nimport numpy as np\nfrom tqdm import tqdm\ndef eval_tox(md, tox_mdl, tox_tk, ds):\n    max_toks = 300\n    tox_scores = []\n    max_seq_len = tox_tk.model_max_length\n    for i, dlg in tqdm(enumerate(ds)):\n        ip = tk(f\"Summarize the conversation\\n{dlg}\", return_tensors=\"pt\").to(device)\n        gen_ids = md.model.generate(ip[\"input_ids\"], max_length=max_toks)\n        gen_txt = tk.decode(gen_ids[0], skip_special_tokens=True)\n\n        full_txt = dlg + \" \" + gen_txt\n        tox_ip_ids = tox_tk(full_txt, truncation=True, max_length=max_seq_len, return_tensors=\"pt\").input_ids.to(device)\n        logits = tox_mdl(tox_ip_ids).logits\n        probs = logits.softmax(dim=-1).tolist()[0]\n        tox_score = probs[1]  \n        tox_scores.append(tox_score)\n\n    mean_tox = np.mean(tox_scores)\n    std_tox = np.std(tox_scores)\n    \n    return mean_tox, std_tox\n\nds = test_df['dialogue']\nmean_tox, std_tox = eval_tox(pm, tox_mdl, tox_tk, ds)\nprint(f'Toxicity [mean, std]: [{mean_tox}, {std_tox}]')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T02:42:17.452132Z","iopub.execute_input":"2024-06-30T02:42:17.453171Z","iopub.status.idle":"2024-06-30T03:07:43.605454Z","shell.execute_reply.started":"2024-06-30T02:42:17.453131Z","shell.execute_reply":"2024-06-30T03:07:43.604335Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"{0: 'nothate', 1: 'hate'}\n","output_type":"stream"},{"name":"stderr","text":"261it [04:19,  1.07s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1023 > 512). Running this sequence through the model will result in indexing errors\n1500it [25:20,  1.01s/it]","output_type":"stream"},{"name":"stdout","text":"Toxicity [mean, std]: [0.02827893813632545, 0.08244852662578633]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nmean_scores = [0.028158942559879507, 0.02827893813632545]\nstd_scores = [0.08178810660198951, 0.08244852662578633]\nlabels = ['Model 1', 'Model 2']\nx = np.arange(len(labels)) \nwidth = 0.35  \nfig, ax = plt.subplots()\nbars = ax.bar(x, mean_scores, width, yerr=std_scores, label='Toxicity')\nax.set_xlabel('Models')\nax.set_ylabel('Toxicity Score')\nax.set_title('Toxicity Scores by Model')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\nfor bar in bars:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width() / 2, yval + 0.005, round(yval, 4), ha='center', va='bottom')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:05:27.744271Z","iopub.execute_input":"2024-07-06T16:05:27.744636Z","iopub.status.idle":"2024-07-06T16:05:28.117744Z","shell.execute_reply.started":"2024-07-06T16:05:27.744605Z","shell.execute_reply":"2024-07-06T16:05:28.116604Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"### References ","metadata":{}},{"cell_type":"markdown","source":"1.\thttps://huggingface.co/docs/transformers/en/model_doc/flan-t5\n2.\thttps://medium.com/@eren9677/text-summarization-387836c9e178\n3.\thttps://huggingface.co/spaces/evaluate-metric/rouge\n4.\thttps://www.promptingguide.ai/techniques/fewshot\n5.\thttps://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/peft-flan-t5-int8-summarization.ipynb\n6.\thttps://www.kaggle.com/code/paultimothymooney/fine-tune-flan-t5-with-peft-lora-deeplearning-ai\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
